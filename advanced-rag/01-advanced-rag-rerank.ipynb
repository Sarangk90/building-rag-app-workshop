{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Workshop Notebook - Naive RAG with Cohere Reranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (25.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wikipedia in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: mwparserfromhell in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (0.7.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (4.13.4)\n",
      "Requirement already satisfied: openai in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.97.0)\n",
      "Requirement already satisfied: qdrant-client in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.14.3)\n",
      "Requirement already satisfied: tqdm in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dotenv in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.1.1)\n",
      "Collecting cohere\n",
      "  Downloading cohere-5.16.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from wikipedia) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.7.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from beautifulsoup4) (4.14.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (1.73.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (2.3.1)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (6.31.1)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
      "  Using cached fastavro-1.11.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (5.5 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting tokenizers<1,>=0.15 (from cohere)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
      "  Using cached types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from tokenizers<1,>=0.15->cohere) (0.33.4)\n",
      "Requirement already satisfied: filelock in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (1.1.5)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Downloading cohere-5.16.1-py3-none-any.whl (291 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached fastavro-1.11.1-cp311-cp311-macosx_10_9_universal2.whl (957 kB)\n",
      "Using cached tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: types-requests, httpx-sse, fastavro, tokenizers, cohere\n",
      "\u001b[2K  Attempting uninstall: httpx-sse\n",
      "\u001b[2K    Found existing installation: httpx-sse 0.4.1\n",
      "\u001b[2K    Uninstalling httpx-sse-0.4.1:\n",
      "\u001b[2K      Successfully uninstalled httpx-sse-0.4.1\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [cohere]2m4/5\u001b[0m [cohere]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cohere-5.16.1 fastavro-1.11.1 httpx-sse-0.4.0 tokenizers-0.21.2 types-requests-2.32.4.20250611\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install wikipedia mwparserfromhell beautifulsoup4 openai qdrant-client tqdm python-dotenv cohere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Qdrant Cloud\n",
      "📚 Collection: workshop_wikipedia_extended\n",
      "🤖 Embedding model: text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Initialize Qdrant Cloud client\n",
    "qdrant_client = QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_URL\"),\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "\n",
    "# Collection configuration\n",
    "collection_name = \"workshop_wikipedia_extended\"\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "\n",
    "print(f\"✅ Connected to Qdrant Cloud\")\n",
    "print(f\"📚 Collection: {collection_name}\")\n",
    "print(f\"🤖 Embedding model: {embedding_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Verify Collection and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Collection Statistics:\n",
      "   Total chunks: 1,210\n",
      "   Vector dimension: 1536\n",
      "   Distance metric: Cosine\n",
      "\n",
      "📝 Sample data structure:\n",
      "\n",
      "Chunk 1:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: Bidirectional encoder representations from transformers (BERT) is a language model introduced in Oct...\n",
      "   Chunk 1 of 10\n",
      "\n",
      "Chunk 2:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: Euclidean space. Encoder: a stack of Transformer blocks with self-attention, but without causal mask...\n",
      "   Chunk 2 of 10\n",
      "\n",
      "Chunk 3:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: consists of a sinusoidal function that takes the position in the sequence as input. Segment type: Us...\n",
      "   Chunk 3 of 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get collection information\n",
    "collection_info = qdrant_client.get_collection(collection_name)\n",
    "point_count = collection_info.points_count\n",
    "\n",
    "print(f\"📊 Collection Statistics:\")\n",
    "print(f\"   Total chunks: {point_count:,}\")\n",
    "print(f\"   Vector dimension: {collection_info.config.params.vectors.size}\")\n",
    "print(f\"   Distance metric: {collection_info.config.params.vectors.distance}\")\n",
    "\n",
    "# Sample a few points to see the data structure\n",
    "sample_points = qdrant_client.scroll(\n",
    "    collection_name=collection_name,\n",
    "    limit=3,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")[0]\n",
    "\n",
    "print(f\"\\n📝 Sample data structure:\")\n",
    "for i, point in enumerate(sample_points):\n",
    "    payload = point.payload\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"   Title: {payload.get('title', 'Unknown')}\")\n",
    "    print(f\"   Text preview: {payload.get('text', '')[:100]}...\")\n",
    "    print(f\"   Chunk {payload.get('chunk_index', 0)+1} of {payload.get('total_chunks', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Q/A Chatbot\n",
    "\n",
    "![../imgs/naive-rag.png](../imgs/naive-rag.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Retrieval - Search the database for the most relevant embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_index': 0,\n",
      " 'text': 'In machine learning, deep learning focuses on utilizing multilayered '\n",
      "         'neural networks to perform tasks such as classification, regression, '\n",
      "         'and representation learning. The field takes inspiration from '\n",
      "         'biological neuroscience and is centered around stacking artificial '\n",
      "         'neurons into layers and \"training\" them to process data. The '\n",
      "         'adjective \"deep\" refers to the use of multiple layers (ranging from '\n",
      "         'three to several hundred or thousands) in the network. Methods used '\n",
      "         'can be supervised, semi-supervised or unsupervised. Some common deep '\n",
      "         'learning network architectures include fully connected networks, '\n",
      "         'deep belief networks, recurrent neural networks, convolutional '\n",
      "         'neural networks, generative adversarial networks, transformers, and '\n",
      "         'neural radiance fields. These architectures have been applied to '\n",
      "         'fields including computer vision, speech recognition, natural '\n",
      "         'language processing, machine translation, bioinformatics, drug '\n",
      "         'design, medical image analysis, climate science, material inspection '\n",
      "         'and board game programs, where they have produced results comparable '\n",
      "         'to and in some cases surpassing human expert performance. Early '\n",
      "         'forms of neural networks were inspired by information processing and '\n",
      "         'distributed communication nodes in biological systems, particularly '\n",
      "         'the human brain. However, current neural networks do not intend to '\n",
      "         'model the brain function of organisms, and are generally seen as '\n",
      "         'low-quality models for that purpose. Overview Most modern deep '\n",
      "         'learning models are based on multi-layered neural networks such as '\n",
      "         'convolutional neural networks and transformers, although they can '\n",
      "         'also include propositional formulas or latent variables organized '\n",
      "         'layer-wise in deep generative models such as the nodes in deep '\n",
      "         'belief networks and deep Boltzmann machines. Fundamentally, deep '\n",
      "         'learning refers to a class of machine learning algorithms in which a '\n",
      "         'hierarchy of layers is used to transform input data into a '\n",
      "         'progressively more abstract and composite representation. For '\n",
      "         'example, in an image recognition model, the raw input may be an '\n",
      "         'image (represented as a tensor of pixels). The first '\n",
      "         'representational layer may attempt to identify basic shapes such as',\n",
      " 'title': 'Deep learning',\n",
      " 'total_chunks': 34,\n",
      " 'url': 'https://en.wikipedia.org/wiki/Deep_learning'}\n"
     ]
    }
   ],
   "source": [
    "# Function to search the database\n",
    "def vector_search(query, top_k=1):\n",
    "    # create embedding of the query\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    query_embeddings = response.data[0].embedding\n",
    "    # similarity search using the embedding, give top n results which are close to the query embeddings\n",
    "    search_result = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embeddings,\n",
    "        with_payload=True,\n",
    "        limit=top_k,\n",
    "    ).points\n",
    "    return [result.payload for result in search_result]\n",
    "\n",
    "\n",
    "search_result = vector_search(\"What does the word 'deep' in 'deep learning' refer\")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(search_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generation - Use the retrieved embeddings to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generate(prompt, model=\"gpt-4.1-nano\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def prompt_template(question, context):\n",
    "    return \"\"\"You are a AI Assistant that provides answer to the question at the end, over the following\n",
    "  pieces of context. Make sure to only use the context to answer the question. Keep the wording very close to the context.\n",
    "  Explicitly mention you DONT KNOW if the answer is not in the context. Answering questions when the answers are not in the context is NOT allowed.\n",
    "  context:\n",
    "  ```\n",
    "  \"\"\" + json.dumps(context) + \"\"\"\n",
    "  ```\n",
    "  User question: \"\"\" + question + \"\"\"\n",
    "  Answer in markdown:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    #Retrieval: search a knowledge base.\n",
    "    search_result = vector_search(question)\n",
    "\n",
    "    prompt = prompt_template(question, search_result)\n",
    "    # Generation: LLMs' ability to generate the answer\n",
    "    return model_generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ```markdown\n",
      "The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "question = f\"Who introduced the time delay neural network (TDNN)? and when ?\"\n",
    "answer = generate_answer(question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG Evaluation with RAGAS\n",
    "\n",
    "Before proceeding with improvements, let's establish baseline scores using **RAGAS** (Retrieval Augmented Generation Assessment Suite) - a specialized framework designed specifically for evaluating RAG systems.\n",
    "\n",
    "### Context-Focused Metrics:\n",
    "\n",
    "1. **Context Precision**: How well are relevant chunks ranked at the top?\n",
    "2. **Context Recall**: How much of the necessary information was retrieved?\n",
    "3. **Context Relevancy**: How relevant is the retrieved context to the question?\n",
    "\n",
    "We're using **RAGAS** because it's purpose-built for RAG evaluation and provides deep insights into context quality - the most critical component of RAG performance. The evaluation is simple to use - just call one function!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Evaluating your Naive RAG system with RAGAS...\n",
      "This will evaluate context quality metrics on 15 questions...\n",
      "\n",
      "✅ Loaded 14 questions from evaluation dataset\n",
      "\n",
      "Evaluating 14 questions...\n",
      "\n",
      "Question 1/14: Who introduced the ReLU (rectified linear unit) ac...\n",
      "Question 2/14: What was the first working deep learning algorithm...\n",
      "Question 3/14: Which CNN achieved superhuman performance in a vis...\n",
      "Question 4/14: When was BERT introduced and by which organization...\n",
      "Question 5/14: What are the two model sizes BERT was originally i...\n",
      "Question 6/14: What percentage of tokens are randomly selected fo...\n",
      "Question 7/14: Who introduced the term 'deep learning' to the mac...\n",
      "Question 8/14: Which three researchers were awarded the 2018 Turi...\n",
      "Question 9/14: When was the first GPT introduced and by which org...\n",
      "Question 10/14: What were the three parameter sizes of the first v...\n",
      "Question 11/14: What is the 'one in ten rule' in regression analys...\n",
      "Question 12/14: What is the essence of overfitting according to th...\n",
      "Question 13/14: In which year and paper was the modern version of ...\n",
      "Question 14/14: What value did the original Transformer paper use ...\n",
      "\n",
      "🔍 Running RAGAS evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 14/14 [00:03<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAGAS EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "CONTEXT RECALL METRIC (0.0 - 1.0 scale):\n",
      "  🟡 Context Recall: 0.643\n",
      "\n",
      "🟡 GOOD: Your context retrieval is working well.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import the RAGAS evaluation utility\n",
    "# Import the RAGAS evaluation utility\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../naive-rag')\n",
    "from rag_evaluator_v2 import evaluate_naive_rag_v2\n",
    "\n",
    "# Run evaluation on the current RAG system using RAGAS\n",
    "print(\"🔍 Evaluating your Naive RAG system with RAGAS...\")\n",
    "print(\"This will evaluate context quality metrics on 15 questions...\\n\")\n",
    "\n",
    "baseline_results = evaluate_naive_rag_v2(\n",
    "    vector_search_func=vector_search,\n",
    "    generate_answer_func=generate_answer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 BASELINE CONTEXT METRICS SUMMARY:\n",
      "Context Recall: 0.643\n",
      "\n",
      "💡 What these RAGAS metrics mean:\n",
      "• Context Recall: How much of the necessary information was retrieved\n",
      "\n",
      "🎯 Score Interpretation:\n",
      "• 0.8+ = Excellent\n",
      "• 0.6-0.8 = Good\n",
      "• 0.4-0.6 = Needs Improvement\n",
      "• <0.4 = Poor\n"
     ]
    }
   ],
   "source": [
    "# Store baseline scores for comparison later\n",
    "baseline_scores = baseline_results.get('aggregate_scores', {})\n",
    "\n",
    "print(\"📊 BASELINE CONTEXT METRICS SUMMARY:\")\n",
    "if 'context_recall' in baseline_scores:\n",
    "    print(f\"Context Recall: {baseline_scores['context_recall']:.3f}\")\n",
    "\n",
    "print(\"\\n💡 What these RAGAS metrics mean:\")\n",
    "print(\"• Context Recall: How much of the necessary information was retrieved\")\n",
    "\n",
    "print(\"\\n🎯 Score Interpretation:\")\n",
    "print(\"• 0.8+ = Excellent\")\n",
    "print(\"• 0.6-0.8 = Good\") \n",
    "print(\"• 0.4-0.6 = Needs Improvement\")\n",
    "print(\"• <0.4 = Poor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📋 Why We Need These Baseline Scores\n",
    "\n",
    "These **RAGAS-powered** baseline scores are crucial because:\n",
    "\n",
    "1. **Context Quality Focus**: RAGAS specifically measures how well your retrieval system finds and ranks relevant information\n",
    "2. **Purpose-Built for RAG**: Unlike general evaluation tools, RAGAS is designed specifically for RAG systems\n",
    "3. **Objective Measurement**: Quantitative metrics that measure actual retrieval performance\n",
    "4. **Debugging Aid**: Low context scores immediately tell you where your RAG is failing\n",
    "5. **Optimization Guide**: Use these metrics to systematically improve your retrieval strategy\n",
    "\n",
    "🔬 **What makes RAGAS special**: \n",
    "- **Context Precision** helps ensure the most relevant information appears first\n",
    "- **Context Recall** ensures you're not missing important information\n",
    "- **Context Relevancy** validates that retrieved chunks actually help answer the question\n",
    "\n",
    "**Next Steps**: Now that we have our baseline context metrics, let's improve our RAG system with Cohere's reranking!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improving RAG with Cohere Reranking\n",
    "\n",
    "Now let's see how adding a reranking step can improve our context selection and overall RAG performance.\n",
    "\n",
    "### Why Reranking?\n",
    "\n",
    "While vector similarity search is good at finding semantically related content, it has limitations:\n",
    "- **Bi-encoder limitation**: Vector embeddings compress all information into a fixed-size representation\n",
    "- **Lost nuances**: Subtle relevance signals can be lost in the embedding process\n",
    "- **No query-document interaction**: Embeddings are created independently\n",
    "\n",
    "Reranking solves these issues by:\n",
    "- **Cross-encoder architecture**: Processes query and document together\n",
    "- **Fine-grained relevance**: Captures subtle semantic relationships\n",
    "- **Better precision**: Filters out less relevant results even if they have high vector similarity\n",
    "\n",
    "### 4.1. Initialize Cohere Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cohere client initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import os\n",
    "\n",
    "# Initialize Cohere client\n",
    "cohere_client = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "print(\"✅ Cohere client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create Reranking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(query, search_results, top_k=3, max_retries=5, initial_backoff=10):\n",
    "    \"\"\"\n",
    "    Rerank search results using Cohere's rerank model with rate limit handling.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's question\n",
    "        search_results: List of search results from vector search\n",
    "        top_k: Number of top results to return after reranking\n",
    "        max_retries: Maximum number of retry attempts for rate-limited requests\n",
    "        initial_backoff: Initial backoff time in seconds (will increase exponentially)\n",
    "    \n",
    "    Returns:\n",
    "        List of reranked results\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Extract texts from search results\n",
    "    documents = [result.get('text', '') for result in search_results]\n",
    "    \n",
    "    # Implement retry with exponential backoff for rate limit handling\n",
    "    retry_count = 0\n",
    "    backoff_time = initial_backoff\n",
    "    \n",
    "    while retry_count <= max_retries:\n",
    "        try:\n",
    "            # Call Cohere rerank\n",
    "            rerank_response = cohere_client.rerank(\n",
    "                query=query,\n",
    "                documents=documents,\n",
    "                model='rerank-english-v3.0',  # Latest rerank model\n",
    "                top_n=top_k\n",
    "            )\n",
    "            \n",
    "            # Return reranked results maintaining original structure\n",
    "            reranked_results = []\n",
    "            for result in rerank_response.results:\n",
    "                original_result = search_results[result.index].copy()\n",
    "                original_result['rerank_score'] = result.relevance_score\n",
    "                reranked_results.append(original_result)\n",
    "            \n",
    "            return reranked_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Check if it's a rate limit error (429)\n",
    "            if hasattr(e, 'status_code') and e.status_code == 429:\n",
    "                if retry_count < max_retries:\n",
    "                    print(f\"⚠️ Rate limit reached. Waiting for {backoff_time} seconds before retrying...\")\n",
    "                    time.sleep(backoff_time)\n",
    "                    # Exponential backoff\n",
    "                    backoff_time *= 2\n",
    "                    retry_count += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"❌ Maximum retries ({max_retries}) reached. Falling back to vector search results.\")\n",
    "            else:\n",
    "                print(f\"❌ Error during reranking: {str(e)}\")\n",
    "            \n",
    "            # Fallback: return the top_k results from the original search\n",
    "            print(\"⚠️ Using original vector search results without reranking.\")\n",
    "            return search_results[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Create Enhanced Answer Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_rerank(question, initial_top_k=10, final_top_k=1):\n",
    "    \"\"\"\n",
    "    Generate answer using reranked results.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        initial_top_k: Number of candidates to retrieve from vector search\n",
    "        final_top_k: Number of results to keep after reranking\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieval - get more candidates\n",
    "    search_results = vector_search(question, top_k=initial_top_k)\n",
    "    \n",
    "    # Step 2: Reranking - select best results\n",
    "    reranked_results = rerank_results(question, search_results, top_k=final_top_k)\n",
    "    \n",
    "    # Step 3: Generation - use reranked results\n",
    "    prompt = prompt_template(question, reranked_results)\n",
    "    return model_generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Compare Results: Naive RAG vs. Reranked RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON: Naive RAG vs. Reranked RAG\n",
      "================================================================================\n",
      "\n",
      "Question: In which year and paper was the modern version of the transformer proposed?\n",
      "\n",
      "=== Without Reranking (Top 3 by vector similarity) ===\n",
      "\n",
      "1. In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a ...\n",
      "\n",
      "2. In 1884, Sir Charles Parsons invented the steam turbine allowing for more efficient electric power generation. Alternating current, with its ability to transmit power more efficiently over long distan...\n",
      "\n",
      "3. keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its pa...\n",
      "\n",
      "📝 Answer (Naive RAG):\n",
      "```markdown\n",
      "The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google.\n",
      "```\n",
      "\n",
      "================================================================================\n",
      "\n",
      "=== With Cohere Reranking (Top 3 from 10 candidates) ===\n",
      "\n",
      "1. [Rerank Score: 1.000] In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a ...\n",
      "\n",
      "2. [Rerank Score: 0.964] to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the \"degradation\" problem. In 2015, two techniques were developed to train very deep networks: the highwa...\n",
      "\n",
      "3. [Rerank Score: 0.787] keep its text processing performance. This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its pa...\n",
      "\n",
      "📝 Answer (Reranked RAG):\n",
      "```markdown\n",
      "The modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Let's compare results on a challenging question\n",
    "# test_question = \"What is the vanishing gradient problem and how does it affect deep neural networks?\"\n",
    "# test_question = \"When was BERT introduced and by which organization?\"\n",
    "test_question = \"In which year and paper was the modern version of the transformer proposed?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON: Naive RAG vs. Reranked RAG\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nQuestion: {test_question}\\n\")\n",
    "\n",
    "print(\"=== Without Reranking (Top 3 by vector similarity) ===\")\n",
    "search_results = vector_search(test_question, top_k=3)\n",
    "for i, result in enumerate(search_results):\n",
    "    print(f\"\\n{i+1}. {result['text'][:200]}...\")\n",
    "\n",
    "answer_naive = generate_answer(test_question)\n",
    "print(f\"\\n📝 Answer (Naive RAG):\\n{answer_naive}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"=== With Cohere Reranking (Top 3 from 10 candidates) ===\")\n",
    "search_results_extended = vector_search(test_question, top_k=10)\n",
    "reranked_results = rerank_results(test_question, search_results_extended, top_k=3)\n",
    "\n",
    "for i, result in enumerate(reranked_results):\n",
    "    print(f\"\\n{i+1}. [Rerank Score: {result['rerank_score']:.3f}] {result['text'][:200]}...\")\n",
    "\n",
    "answer_reranked = generate_answer_with_rerank(test_question)\n",
    "print(f\"\\n📝 Answer (Reranked RAG):\\n{answer_reranked}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Evaluate Improvement with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Evaluating RAG system with Cohere Reranking...\n",
      "This will evaluate the improved system on the same 15 questions...\n",
      "\n",
      "✅ Loaded 14 questions from evaluation dataset\n",
      "\n",
      "Evaluating 14 questions...\n",
      "\n",
      "Question 1/14: Who introduced the ReLU (rectified linear unit) ac...\n",
      "Question 2/14: What was the first working deep learning algorithm...\n",
      "Question 3/14: Which CNN achieved superhuman performance in a vis...\n",
      "Question 4/14: When was BERT introduced and by which organization...\n",
      "Question 5/14: What are the two model sizes BERT was originally i...\n",
      "Question 6/14: What percentage of tokens are randomly selected fo...\n",
      "Question 7/14: Who introduced the term 'deep learning' to the mac...\n",
      "Question 8/14: Which three researchers were awarded the 2018 Turi...\n",
      "Question 9/14: When was the first GPT introduced and by which org...\n",
      "⚠️ Rate limit reached. Waiting for 10 seconds before retrying...\n",
      "⚠️ Rate limit reached. Waiting for 20 seconds before retrying...\n",
      "⚠️ Rate limit reached. Waiting for 40 seconds before retrying...\n",
      "Question 10/14: What were the three parameter sizes of the first v...\n",
      "Question 11/14: What is the 'one in ten rule' in regression analys...\n",
      "Question 12/14: What is the essence of overfitting according to th...\n",
      "Question 13/14: In which year and paper was the modern version of ...\n",
      "Question 14/14: What value did the original Transformer paper use ...\n",
      "\n",
      "🔍 Running RAGAS evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 14/14 [00:39<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAGAS EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "CONTEXT RECALL METRIC (0.0 - 1.0 scale):\n",
      "  🟢 Context Recall: 1.000\n",
      "\n",
      "🟢 EXCELLENT: Your context retrieval is highly effective!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation with reranking\n",
    "print(\"🔍 Evaluating RAG system with Cohere Reranking...\")\n",
    "print(\"This will evaluate the improved system on the same 15 questions...\\n\")\n",
    "\n",
    "reranked_results = evaluate_naive_rag_v2(\n",
    "    vector_search_func=lambda q: vector_search(q, top_k=15),\n",
    "    generate_answer_func=generate_answer_with_rerank\n",
    ")\n",
    "\n",
    "# Store reranked scores\n",
    "reranked_scores = reranked_results.get('aggregate_scores', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 IMPROVEMENT WITH RERANKING\n",
      "============================================================\n",
      "\n",
      "Context Recall:\n",
      "  Baseline: 0.643\n",
      "  With Reranking: 1.000\n",
      "  Improvement: +0.357 (+55.6%)\n",
      "\n",
      "============================================================\n",
      "\n",
      "🎉 Key Insights:\n",
      "• Reranking typically improves context precision significantly\n",
      "• Better context selection leads to more accurate answers\n",
      "• The cross-encoder architecture of rerankers captures nuanced relevance\n",
      "• This is especially valuable for complex or ambiguous queries\n"
     ]
    }
   ],
   "source": [
    "# Compare improvements\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📊 IMPROVEMENT WITH RERANKING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for metric in ['context_recall']:\n",
    "    if metric in baseline_scores and metric in reranked_scores:\n",
    "        baseline = baseline_scores[metric]\n",
    "        reranked = reranked_scores[metric]\n",
    "        improvement = reranked - baseline\n",
    "        improvement_pct = (improvement / baseline) * 100 if baseline > 0 else 0\n",
    "        \n",
    "        print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Baseline: {baseline:.3f}\")\n",
    "        print(f\"  With Reranking: {reranked:.3f}\")\n",
    "        print(f\"  Improvement: {improvement:+.3f} ({improvement_pct:+.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\n🎉 Key Insights:\")\n",
    "print(\"• Reranking typically improves context precision significantly\")\n",
    "print(\"• Better context selection leads to more accurate answers\")\n",
    "print(\"• The cross-encoder architecture of rerankers captures nuanced relevance\")\n",
    "print(\"• This is especially valuable for complex or ambiguous queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Next Steps\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **Naive RAG Limitations**: While vector search is effective, it can miss nuanced relevance\n",
    "2. **Reranking Benefits**: Cross-encoder models like Cohere's reranker significantly improve context selection\n",
    "3. **Measurable Improvements**: RAGAS metrics clearly show the performance gains\n",
    "\n",
    "### Architecture Comparison\n",
    "\n",
    "**Naive RAG:**\n",
    "```\n",
    "Query → Embedding → Vector Search (Top 3) → Generate Answer\n",
    "```\n",
    "\n",
    "**Reranked RAG:**\n",
    "```\n",
    "Query → Embedding → Vector Search (Top 10) → Rerank (Top 3) → Generate Answer\n",
    "```\n",
    "\n",
    "### When to Use Reranking\n",
    "\n",
    "✅ **Use reranking when:**\n",
    "- Answer quality is critical\n",
    "- You have complex, nuanced queries\n",
    "- Your corpus contains similar but subtly different content\n",
    "- You can afford the additional API call latency\n",
    "\n",
    "❌ **Skip reranking when:**\n",
    "- Speed is more important than accuracy\n",
    "- Queries are simple and unambiguous\n",
    "- Your corpus has clearly distinct topics\n",
    "\n",
    "### Further Improvements\n",
    "\n",
    "1. **Hybrid Search**: Combine vector search with keyword search\n",
    "2. **Query Expansion**: Generate multiple query variations\n",
    "3. **Document Expansion**: Add metadata and summaries to chunks\n",
    "4. **Fine-tuning**: Train custom rerankers on your domain\n",
    "5. **Caching**: Store reranked results for common queries\n",
    "\n",
    "### Try It Yourself!\n",
    "\n",
    "Experiment with:\n",
    "- Different `initial_top_k` values (try 20, 50)\n",
    "- Different `final_top_k` values (try 5, 7)\n",
    "- Different reranking models\n",
    "- Your own questions and see the improvement!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
