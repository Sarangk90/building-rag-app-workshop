{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f2244f11eab922",
   "metadata": {},
   "source": [
    "# Data indexing\n",
    "\n",
    "## Scifact Dataset\n",
    "\n",
    "\n",
    "The Scifact dataset is a specialized collection of scientific claims and evidence from research papers, designed for scientific fact-checking and verification tasks. It consists of scientific claims paired with abstracts from research papers that either support or refute these claims.\n",
    "\n",
    "The dataset contains over 5,000 scientific abstracts from research papers across various scientific domains including medicine, biology, chemistry, and other life sciences. Each entry in the dataset includes a unique ID, the paper's title, and the full text of the abstract.\n",
    "\n",
    "Originally created to help evaluate scientific claim verification systems, this dataset is part of the Benchmark for Scientific Claim Verification (BeIR) collection. It's particularly useful for building scientific fact-checking systems, training models to understand and verify scientific claims, and developing information retrieval systems for scientific literature.\n",
    "\n",
    "Let's explore the dataset structure and prepare it for our RAG application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d321e6755b15e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"BeIR/scifact\", \"corpus\", split=\"corpus\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26797f7a57adc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976c453bf0d27c0",
   "metadata": {},
   "source": [
    "## Dense embeddings\n",
    "\n",
    "We're not going to choose the fanciest embedding model out there, but stick to something simple and efficient. FastEmbed comes with some pretrained models that we can use out of the box. Due to ONNX usage, these models can be launched efficiently even on a CPU. The `all-MiniLM-L6-v2` model is a lightweight model that's good for a start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ad98114d793b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding\n",
    "\n",
    "dense_embedding_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "dense_embeddings = list(dense_embedding_model.passage_embed(dataset[\"text\"][0:1]))\n",
    "len(dense_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702cbdb3d2c6a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dense_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74386f75bb96941",
   "metadata": {},
   "source": [
    "## Sparse embeddings\n",
    "\n",
    "Similarly, we can use a BM25 model to generate sparse embeddings, so it hopefully will handle the cases in which the dense embeddings fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc60aa2d0956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import SparseTextEmbedding\n",
    "\n",
    "bm25_embedding_model = SparseTextEmbedding(\"Qdrant/bm25\")\n",
    "bm25_embeddings = list(bm25_embedding_model.passage_embed(dataset[\"text\"][0:1]))\n",
    "bm25_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4fbac945e35a55",
   "metadata": {},
   "source": [
    "## Putting data in a Qdrant collection\n",
    "\n",
    "All the vectors might be now upserted into a Qdrant collection. Keeping them all in a single one enables the possibility to combine different embeddings and create even a complex pipeline with several steps. Depending on the specifics of your data, you may prefer to use a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ec613-ad0c-4a35-b0da-bb2f69294999",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant:v1.10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ba5b8b767ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\", timeout=600)\n",
    "client.create_collection(\n",
    "    \"scifact\",\n",
    "    vectors_config={\n",
    "        \"all-MiniLM-L6-v2\": models.VectorParams(\n",
    "            size=len(dense_embeddings[0]),\n",
    "            distance=models.Distance.COSINE,\n",
    "        )\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238650e7e3ea136e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "batch_size = 5\n",
    "for batch in tqdm.tqdm(dataset.iter(batch_size=batch_size), \n",
    "                       total=len(dataset) // batch_size):\n",
    "    dense_embeddings = list(dense_embedding_model.passage_embed(batch[\"text\"]))\n",
    "    bm25_embeddings = list(bm25_embedding_model.passage_embed(batch[\"text\"]))\n",
    "    \n",
    "    client.upload_points(\n",
    "        \"scifact\",\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=int(batch[\"_id\"][i]),\n",
    "                vector={\n",
    "                    \"all-MiniLM-L6-v2\": dense_embeddings[i].tolist(),\n",
    "                    \"bm25\": bm25_embeddings[i].as_object(),\n",
    "                },\n",
    "                payload={\n",
    "                    \"_id\": batch[\"_id\"][i],\n",
    "                    \"title\": batch[\"title\"][i],\n",
    "                    \"text\": batch[\"text\"][i],\n",
    "                }\n",
    "            )\n",
    "            for i, _ in enumerate(batch[\"_id\"])\n",
    "        ],\n",
    "        # We send a lot of embeddings at once, so it's best to reduce the batch size.\n",
    "        # Otherwise, we would have gigantic requests sent for each batch and we can\n",
    "        # easily reach the maximum size of a single request.\n",
    "        batch_size=batch_size,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae256b5-b874-41e4-b633-85c16f29f2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.recover_snapshot(\n",
    "#     \"scifact\",\n",
    "#     location=\"https://storage.googleapis.com/common-datasets-snapshots/scifact-multiple-representations.snapshot\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8154fe8e865a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_collection(\"scifact\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
