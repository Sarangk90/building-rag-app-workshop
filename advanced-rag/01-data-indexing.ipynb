{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f2244f11eab922",
   "metadata": {},
   "source": [
    "# Data indexing\n",
    "\n",
    "## Scifact Dataset\n",
    "\n",
    "\n",
    "The Scifact dataset is a specialized collection of scientific claims and evidence from research papers, designed for scientific fact-checking and verification tasks. It consists of scientific claims paired with abstracts from research papers that either support or refute these claims.\n",
    "\n",
    "The dataset contains over 5,000 scientific abstracts from research papers across various scientific domains including medicine, biology, chemistry, and other life sciences. Each entry in the dataset includes a unique ID, the paper's title, and the full text of the abstract.\n",
    "\n",
    "Originally created to help evaluate scientific claim verification systems, this dataset is part of the Benchmark for Scientific Claim Verification (BeIR) collection. It's particularly useful for building scientific fact-checking systems, training models to understand and verify scientific claims, and developing information retrieval systems for scientific literature.\n",
    "\n",
    "Let's explore the dataset structure and prepare it for our RAG application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "5d321e6755b15e1a",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"BeIR/scifact\", \"corpus\", split=\"corpus\")\n",
    "dataset[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Description\n",
    "- **Loading the Dataset:** We use the `datasets` library to load the \"corpus\" split of the Scifact dataset from the BeIR collection. This split contains the abstracts we’ll index.\n",
    "- **Inspecting a Sample:** `dataset[0]` retrieves the first entry, showing its structure: a dictionary with keys `_id` (unique identifier), `title` (paper title), and `text` (abstract text).\n",
    "- **Purpose:** This step helps us understand the data we’re working with, confirming it matches the expected format for indexing.\n",
    "\n",
    "Next, let’s check the total number of documents in the dataset.\n"
   ],
   "id": "2568cad43ca7a86"
  },
  {
   "cell_type": "code",
   "id": "26797f7a57adc4ca",
   "metadata": {},
   "source": [
    "len(dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f976c453bf0d27c0",
   "metadata": {},
   "source": [
    "## Dense Embeddings\n",
    "\n",
    "Dense embeddings capture the semantic meaning of text, allowing searches based on concepts rather than just exact keywords. For this notebook, we’re not going to choose the fanciest embedding model out there, but stick to something simple and efficient. FastEmbed provides pretrained models that we can use out of the box. Due to ONNX usage, these models can be launched efficiently even on a CPU. The `all-MiniLM-L6-v2` model is a lightweight model from Sentence Transformers that’s good for a start.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "611ad98114d793b9",
   "metadata": {},
   "source": [
    "from fastembed import TextEmbedding\n",
    "\n",
    "dense_embedding_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "dense_embeddings = list(dense_embedding_model.passage_embed(dataset[\"text\"][0:1]))\n",
    "len(dense_embeddings)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Description\n",
    "- **Model Initialization:** We load the `all-MiniLM-L6-v2` model using `TextEmbedding` from FastEmbed. This model is optimized for semantic text representation and is lightweight, making it suitable for CPU-based environments.\n",
    "- **Generating Embeddings:** We embed the text of the first abstract (`dataset[\"text\"][0:1]`) to test the process. The result is a list of dense embedding vectors.\n",
    "- **Output Check:** `len(dense_embeddings)` confirms we get one embedding vector for the single document processed.\n",
    "\n",
    "Let’s inspect the dimensionality of the dense embeddings."
   ],
   "id": "a9d222dc8f36308d"
  },
  {
   "cell_type": "code",
   "id": "702cbdb3d2c6a32a",
   "metadata": {},
   "source": [
    "len(dense_embeddings[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Description\n",
    "- **Vector Dimensionality:** This returns the length of the embedding vector (e.g., 384 dimensions for `all-MiniLM-L6-v2`).\n",
    "- **Significance:** The dimensionality is crucial for configuring the Qdrant collection later, as it defines the size of the vector space we’ll store and search."
   ],
   "id": "f012c54fa59f50ca"
  },
  {
   "cell_type": "markdown",
   "id": "b74386f75bb96941",
   "metadata": {},
   "source": [
    "## Sparse Embeddings\n",
    "\n",
    "Sparse embeddings, like those generated by BM25, are effective for keyword-based searches, capturing exact term matches rather than semantic similarity. Similarly, we can use a BM25 model to generate sparse embeddings, so it hopefully will handle the cases in which the dense embeddings fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "70bc60aa2d0956d",
   "metadata": {},
   "source": [
    "from fastembed import SparseTextEmbedding\n",
    "\n",
    "bm25_embedding_model = SparseTextEmbedding(\"Qdrant/bm25\")\n",
    "bm25_embeddings = list(bm25_embedding_model.passage_embed(dataset[\"text\"][0:1]))\n",
    "bm25_embeddings\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Description\n",
    "- **Model Initialization:** We load the `Qdrant/bm25` model using `SparseTextEmbedding`. BM25 is a traditional ranking algorithm that scores documents based on term frequency and inverse document frequency.\n",
    "- **Generating Embeddings:** We embed the first abstract’s text to produce a sparse vector, which highlights important keywords with non-zero values while most elements remain zero.\n",
    "- **Output Inspection:** `bm25_embeddings` shows the sparse vector structure, typically as a list of dictionaries with indices and values for non-zero terms.\n",
    "- **Complementary Role:** Sparse embeddings complement dense embeddings by excelling in exact-match scenarios, enhancing retrieval robustness.\n"
   ],
   "id": "20dd23cb10438bc3"
  },
  {
   "cell_type": "markdown",
   "id": "ed4fbac945e35a55",
   "metadata": {},
   "source": [
    "## Putting Data in a Qdrant Collection\n",
    "\n",
    "All the vectors might be now upserted into a Qdrant collection. Keeping them all in a single one enables the possibility to combine different embeddings and create even a complex pipeline with several steps. Depending on the specifics of your data, you may prefer to use a different approach. Qdrant is a vector database optimized for storing and searching high-dimensional data efficiently.\n",
    "\n",
    "### Starting Qdrant with Docker\n",
    "\n",
    "First, let’s set up a Qdrant instance if it’s not already running.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "431ec613-ad0c-4a35-b0da-bb2f69294999",
   "metadata": {},
   "source": "!docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant:v1.13.2\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Description\n",
    "- **Docker Command:** This runs Qdrant version 1.13.2 in a detached mode (`-d`) and maps ports 6333 (REST API) and 6334 (gRPC) from the container to your local machine.\n",
    "- **Purpose:** Ensures a Qdrant server is available locally to store and manage our embeddings.\n",
    "\n",
    "### Creating the Qdrant Collection\n",
    "\n",
    "Now, let’s configure a collection to store both dense and sparse embeddings."
   ],
   "id": "832bc6dec9436059"
  },
  {
   "cell_type": "code",
   "id": "1f0ba5b8b767ae5b",
   "metadata": {},
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\", timeout=600)\n",
    "client.create_collection(\n",
    "    \"scifact\",\n",
    "    vectors_config={\n",
    "        \"all-MiniLM-L6-v2\": models.VectorParams(\n",
    "            size=len(dense_embeddings[0]),\n",
    "            distance=models.Distance.COSINE,\n",
    "\n",
    "        )\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Description\n",
    "- **Client Setup:** We connect to the local Qdrant instance with a generous timeout (600 seconds) to handle large uploads.\n",
    "- **Collection Creation:** We create a collection named \"scifact\" with configurations for:\n",
    "  - **Dense Vectors:** Named `all-MiniLM-L6-v2`, with the vector size from our earlier embedding (e.g., 384) and Cosine distance for similarity searches.\n",
    "  - **Sparse Vectors:** Named `bm25`, using an IDF (Inverse Document Frequency) modifier to weight terms based on their rarity across the dataset.\n",
    "- **Why Combined Storage:** Storing both embedding types in one collection enables hybrid search capabilities later.\n",
    "\n",
    "### Uploading Data\n",
    "\n",
    "We’ll upload the dataset’s embeddings and metadata to Qdrant in batches for efficiency.\n"
   ],
   "id": "fe2d41867ee8aa3d"
  },
  {
   "cell_type": "code",
   "id": "238650e7e3ea136e",
   "metadata": {},
   "source": [
    "import tqdm\n",
    "\n",
    "batch_size = 5\n",
    "for batch in tqdm.tqdm(dataset.iter(batch_size=batch_size), \n",
    "                       total=len(dataset) // batch_size):\n",
    "    dense_embeddings = list(dense_embedding_model.passage_embed(batch[\"text\"]))\n",
    "    bm25_embeddings = list(bm25_embedding_model.passage_embed(batch[\"text\"]))\n",
    "    \n",
    "    client.upload_points(\n",
    "        \"scifact\",\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=int(batch[\"_id\"][i]),\n",
    "                vector={\n",
    "                    \"all-MiniLM-L6-v2\": dense_embeddings[i].tolist(),\n",
    "                    \"bm25\": bm25_embeddings[i].as_object(),\n",
    "                },\n",
    "                payload={\n",
    "                    \"_id\": batch[\"_id\"][i],\n",
    "                    \"title\": batch[\"title\"][i],\n",
    "                    \"text\": batch[\"text\"][i],\n",
    "                }\n",
    "            )\n",
    "            for i, _ in enumerate(batch[\"_id\"])\n",
    "        ],\n",
    "        # We send a lot of embeddings at once, so it's best to reduce the batch size.\n",
    "        # Otherwise, we would have gigantic requests sent for each batch and we can\n",
    "        # easily reach the maximum size of a single request.\n",
    "        batch_size=batch_size,  \n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Description\n",
    "- **Batching:** We process the dataset in chunks of 5 documents (`batch_size=5`) to manage memory and avoid oversized requests. `tqdm` provides a progress bar for tracking.\n",
    "- **Embedding Generation:** For each batch, we compute dense and sparse embeddings for all abstracts in the batch.\n",
    "- **PointStruct:** Each document becomes a \"point\" in Qdrant with:\n",
    "  - `id`: A unique integer ID from `_id`.\n",
    "  - `vector`: A dictionary with dense (`all-MiniLM-L6-v2`) and sparse (`bm25`) embeddings.\n",
    "  - `payload`: Metadata including the ID, title, and text for retrieval purposes.\n",
    "- **Uploading:** `upload_points` sends these points to the \"scifact\" collection, with batching to optimize performance.\n",
    "\n",
    "### Verifying the Collection\n",
    "\n",
    "Let’s confirm the collection is set up correctly.\n"
   ],
   "id": "fd4b092fded5f93d"
  },
  {
   "cell_type": "code",
   "id": "8d8154fe8e865a8",
   "metadata": {},
   "source": [
    "client.get_collection(\"scifact\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7c0a2a073d878910",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
