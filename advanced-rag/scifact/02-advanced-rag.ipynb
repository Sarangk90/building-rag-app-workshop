{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267049883e2e1a93",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced RAG:\n",
    "## Hybrid Search and Reranking with Qdrant and Sentence Transformers\n",
    "The goal of the notebook is to demonstrate how using advanced techniques improve the search quality of a dense retrieval system. We'll combine dense and sparse search methods, then rerank the results using a cross-encoder to enhance relevance and accuracy.\n",
    "\n",
    "## 1. Setting Up the Qdrant Client\n",
    "\n",
    "We begin by connecting to the Qdrant vector database, which stores our indexed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8d1d63f14e4e0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:16:57.032756Z",
     "start_time": "2025-07-28T04:16:57.007403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=5183)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\", timeout=600)\n",
    "client.count(\"scifact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd5262",
   "metadata": {},
   "source": [
    "## 2. Loading Embedding Models\n",
    "\n",
    "We’ll use two embedding models:\n",
    "- **Dense embeddings** for semantic search.\n",
    "- **Sparse embeddings** for keyword-based search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "638b0fdc7b4aeb6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:16:59.731822Z",
     "start_time": "2025-07-28T04:16:57.038788Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding, SparseTextEmbedding\n",
    "\n",
    "dense_embedding_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "bm25_embedding_model = SparseTextEmbedding(\"Qdrant/bm25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e437bea",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- **Dense Embeddings (`all-MiniLM-L6-v2`):** Generated by Sentence Transformers, these capture the semantic meaning of text, allowing searches based on concepts rather than exact words.\n",
    "- **Sparse Embeddings (`BM25`):** A traditional keyword-matching method that excels at finding documents with exact term matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80dcbd0",
   "metadata": {},
   "source": [
    "## 3. Performing a Simple Semantic Search\n",
    "\n",
    "Let’s try a basic search using dense embeddings to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "id": "5048bc77ea2ec3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:23:18.951292Z",
     "start_time": "2025-07-28T04:23:18.948911Z"
    }
   },
   "source": "query_text = \"0-dimensional biomaterials lack inductive properties.\"",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "9d82f3a93b1da702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:23:19.544955Z",
     "start_time": "2025-07-28T04:23:19.532936Z"
    }
   },
   "source": [
    "from qdrant_client.fastembed_common import QueryResponse\n",
    "\n",
    "response: QueryResponse = client.query_points(\n",
    "    \"scifact\",\n",
    "    query=next(dense_embedding_model.query_embed(query_text)),\n",
    "    using=\"all-MiniLM-L6-v2\",\n",
    "    limit=10,\n",
    "    with_payload=True,\n",
    ")\n",
    "\n",
    "len(response.points)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "36db322d7c86dc57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:23:20.054225Z",
     "start_time": "2025-07-28T04:23:20.051573Z"
    }
   },
   "source": "response.points[4]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScoredPoint(id=31715818, version=848, score=0.29541197, payload={'_id': '31715818', 'title': 'New opportunities: the use of nanotechnologies to manipulate and track stem cells.', 'text': 'Nanotechnologies are emerging platforms that could be useful in measuring, understanding, and manipulating stem cells. Examples include magnetic nanoparticles and quantum dots for stem cell labeling and in vivo tracking; nanoparticles, carbon nanotubes, and polyplexes for the intracellular delivery of genes/oligonucleotides and protein/peptides; and engineered nanometer-scale scaffolds for stem cell differentiation and transplantation. This review examines the use of nanotechnologies for stem cell tracking, differentiation, and transplantation. We further discuss their utility and the potential concerns regarding their cytotoxicity.'}, vector=None, shard_key=None, order_value=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "43ebf487",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- We convert the query into a dense vector using the dense embedding model.\n",
    "- The `query_points` method searches the \"scifact\" collection for the top 10 documents closest to this vector in the `all-MiniLM-L6-v2` field.\n",
    "- `with_payload=False` means we get only document IDs and scores, not the full text (for simplicity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d612328da4cec69",
   "metadata": {},
   "source": [
    "## 4. Benchmarking with BeIR SciFact Dataset\n",
    "\n",
    "To evaluate our search methods, we’ll use the **BeIR SciFact dataset**, a standard benchmark for information retrieval.\n",
    "\n",
    "### 4.1 Loading Queries and Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3acd3603a87c1a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:00.643898Z",
     "start_time": "2025-07-28T04:16:59.772063Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since BeIR/scifact couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'queries' at /Users/sarangsanjaykulkarni/.cache/huggingface/datasets/BeIR___scifact/queries/0.0.0/984eed826375f18d27936c4a32bf0f8491e3f414 (last modified on Sun Jul  6 21:25:50 2025).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_id': ['0', '2', '4', '6', '9', '10', '11', '12', '14', '15'],\n",
       " 'title': ['', '', '', '', '', '', '', '', '', ''],\n",
       " 'text': ['0-dimensional biomaterials lack inductive properties.',\n",
       "  '1 in 5 million in UK have abnormal PrP positivity.',\n",
       "  '1-1% of colorectal cancer patients are diagnosed with regional or distant metastases.',\n",
       "  '10% of sudden infant death syndrome (SIDS) deaths happen in newborns aged less than 6 months.',\n",
       "  '32% of liver transplantation programs required patients to discontinue methadone treatment in 2001.',\n",
       "  '4-PBA treatment decreases endoplasmic reticulum stress in response to general endoplasmic reticulum stress markers.',\n",
       "  '4-PBA treatment raises endoplasmic reticulum stress in response to general endoplasmic reticulum stress markers.',\n",
       "  '40mg/day dosage of folic acid and 2mg/day dosage of vitamin B12 does not affect chronic kidney disease (CKD) progression.',\n",
       "  \"5'-nucleotidase metabolizes 6MP.\",\n",
       "  '50% of patients exposed to radiation have activated markers of mesenchymal stem cells.']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "queries_dataset = load_dataset(\"BeIR/scifact\", \"queries\", split=\"queries\")\n",
    "len(queries_dataset)\n",
    "queries_dataset[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f54600bf8e72c215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:06.945604Z",
     "start_time": "2025-07-28T04:17:00.651359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_qrels = load_dataset(\"BeIR/scifact-qrels\", split=\"train\")\n",
    "len(query_qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88ecf1d7eff3e17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:06.957601Z",
     "start_time": "2025-07-28T04:17:06.955047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query-id': [0, 2, 4],\n",
       " 'corpus-id': [31715818, 13734012, 22942787],\n",
       " 'score': [1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_qrels[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07898e5",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- **queries_dataset:** Contains the queries we’ll search with.\n",
    "- **query_qrels:** Provides ground truth relevance labels (query ID, document ID, and score) to evaluate search performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a09e6fe2dc6546",
   "metadata": {},
   "source": [
    "### 4.2 Building the Ground Truth Dataset\n",
    "\n",
    "We need to format the ground truth into a structure suitable for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c2ba8fe89443d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:08.120908Z",
     "start_time": "2025-07-28T04:17:06.971023Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ranx import Qrels\n",
    "from collections import defaultdict\n",
    "\n",
    "qrels_dict = defaultdict(dict)\n",
    "for entry in query_qrels:\n",
    "    query_id = str(entry[\"query-id\"])\n",
    "    doc_id = str(entry[\"corpus-id\"])\n",
    "    qrels_dict[query_id][doc_id] = entry[\"score\"]\n",
    "\n",
    "qrels = Qrels(qrels_dict, name=\"scifact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764f996",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- We create a dictionary where each query ID maps to document IDs and their relevance scores (e.g., 1 for relevant, 0 for irrelevant).\n",
    "- The `Qrels` object from the `ranx` library will be used later to evaluate search results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3eecd7-d8ad-49d0-b9ea-48dd5dcc2d6e",
   "metadata": {},
   "source": [
    "## 5. Precomputing Query Embeddings\n",
    "\n",
    "To speed up testing, we precompute embeddings for all queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4780b98-5721-4869-8dd1-1a2fa7b1343e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:10.584035Z",
     "start_time": "2025-07-28T04:17:09.133911Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1109/1109 [00:01<00:00, 766.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "dense_vectors, sparse_vectors, late_vectors = [], [], []\n",
    "for query in tqdm.tqdm(queries_dataset):\n",
    "    dense_query_vector = next(dense_embedding_model.query_embed(query[\"text\"]))\n",
    "    sparse_query_vector = next(bm25_embedding_model.query_embed(query[\"text\"]))\n",
    "\n",
    "    dense_vectors.append(dense_query_vector)\n",
    "    sparse_vectors.append(sparse_query_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c8a71",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- We loop through each query, generating its dense and sparse embeddings.\n",
    "- `tqdm` shows a progress bar, making it clear how long the process takes.\n",
    "- These precomputed vectors save time when testing multiple search pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29ab53",
   "metadata": {},
   "source": [
    "## 6. Testing Search Pipelines\n",
    "\n",
    "We’ll evaluate three search approaches:\n",
    "1. **Dense embeddings alone**\n",
    "2. **Sparse embeddings alone**\n",
    "3. **Hybrid search with Reciprocal Rank Fusion (RRF)**\n",
    "\n",
    "### 6.1 Dense Embeddings Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1a6612b06d18969",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:13.345313Z",
     "start_time": "2025-07-28T04:17:10.597959Z"
    }
   },
   "outputs": [],
   "source": [
    "from ranx import Run\n",
    "\n",
    "run_dict = {}\n",
    "for query_idx, query in enumerate(queries_dataset):\n",
    "    query_id = str(query[\"_id\"])\n",
    "\n",
    "    query_vector = dense_vectors[query_idx]\n",
    "\n",
    "    results = client.query_points(\n",
    "        \"scifact\",\n",
    "        query=query_vector,\n",
    "        using=\"all-MiniLM-L6-v2\",\n",
    "        with_payload=False,\n",
    "        limit=5,\n",
    "    )\n",
    "\n",
    "    run_dict[query_id] = {\n",
    "        str(point.id): point.score\n",
    "        for point in results.points\n",
    "    }\n",
    "\n",
    "dense_run = Run(run_dict, name=\"semantic_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69cc1517ccb95af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:13.878263Z",
     "start_time": "2025-07-28T04:17:13.655788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision@5': np.float64(0.1517923362175525),\n",
       " 'mrr@5': np.float64(0.5762875978574372)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ranx import evaluate\n",
    "\n",
    "evaluate(qrels, dense_run, metrics=[\"precision@5\", \"mrr@5\"], make_comparable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83bc83c",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- We search using dense vectors and retrieve the top 5 results per query.\n",
    "- Results are stored in a `Run` object for evaluation.\n",
    "- **Precision@5:** Fraction of the top 5 results that are relevant.\n",
    "- **MRR@5:** Average reciprocal rank of the first relevant result in the top 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d70bb18-3719-437f-82c4-28ec0f7b6c0d",
   "metadata": {},
   "source": [
    "### 6.2 Sparse Embeddings Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e720df26-6263-47a6-bbe2-268d40eeda3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:15.357378Z",
     "start_time": "2025-07-28T04:17:13.902061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision@5': np.float64(0.16093943139678615),\n",
       " 'mrr@5': np.float64(0.6474660074165636)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_dict = {}\n",
    "for query_idx, query in enumerate(queries_dataset):\n",
    "    query_id = str(query[\"_id\"])\n",
    "\n",
    "    query_vector = sparse_vectors[query_idx]\n",
    "\n",
    "    results = client.query_points(\n",
    "        \"scifact\",\n",
    "        query=models.SparseVector(**query_vector.as_object()),\n",
    "        using=\"bm25\",\n",
    "        with_payload=False,\n",
    "        limit=5,\n",
    "    )\n",
    "\n",
    "    run_dict[query_id] = {\n",
    "        str(point.id): point.score\n",
    "        for point in results.points\n",
    "    }\n",
    "\n",
    "bm25_run = Run(run_dict, name=\"bm25\")\n",
    "evaluate(qrels, bm25_run, metrics=[\"precision@5\", \"mrr@5\"], make_comparable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f3ded",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- Similar to the dense search, but using sparse vectors (BM25) for keyword-based retrieval.\n",
    "- We evaluate the same metrics to compare performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f42b3-d9e3-4964-82f9-42080f8eb853",
   "metadata": {},
   "source": [
    "### 6.3 Hybrid Search with Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "Hybrid search combines dense and sparse results to improve retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d48c1c9-39bc-4c05-a589-d8dbb18d76b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:19.753444Z",
     "start_time": "2025-07-28T04:17:15.395832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision@5': np.float64(0.17132262051915945),\n",
       " 'mrr@5': np.float64(0.6561186650185415)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_search_run_dict = {}\n",
    "hybrid_search_result = {}\n",
    "for query_idx, query in enumerate(queries_dataset):\n",
    "    query_id = str(query[\"_id\"])\n",
    "\n",
    "    dense_query_vector = dense_vectors[query_idx]\n",
    "    sparse_query_vector = sparse_vectors[query_idx]\n",
    "\n",
    "    prefetch = [\n",
    "        models.Prefetch(\n",
    "            query=dense_query_vector,\n",
    "            using=\"all-MiniLM-L6-v2\",\n",
    "            limit=10,\n",
    "        ),\n",
    "        models.Prefetch(\n",
    "            query=models.SparseVector(**sparse_query_vector.as_object()),\n",
    "            using=\"bm25\",\n",
    "            limit=10,\n",
    "        ),\n",
    "    ]\n",
    "    hybrid_search_result = client.query_points(\n",
    "        \"scifact\",\n",
    "        prefetch=prefetch,\n",
    "        query=models.FusionQuery(\n",
    "            fusion=models.Fusion.RRF,\n",
    "        ),\n",
    "        with_payload=False,\n",
    "        limit=5,\n",
    "    )\n",
    "\n",
    "    hybrid_search_run_dict[query_id] = {\n",
    "        str(point.id): point.score\n",
    "        for point in hybrid_search_result.points\n",
    "    }\n",
    "\n",
    "rrf_run = Run(hybrid_search_run_dict, name=\"hybrid_search\")\n",
    "evaluate(qrels, rrf_run, metrics=[\"precision@5\", \"mrr@5\"], make_comparable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11359dae-a1c4-4f0e-a5b7-16e2bcbfe8be",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- **Hybrid Search:** Combines semantic (dense) and keyword (sparse) searches.\n",
    "- **Prefetch:** Runs both searches in parallel, retrieving the top 10 results from each.\n",
    "- **RRF:** Fuses the rankings by assigning scores based on the reciprocal of each document’s rank (e.g., 1st = 1/1, 2nd = 1/2), then summing them. This balances the methods without needing weights.\n",
    "- We take the top 5 fused results and evaluate them.\n",
    "\n",
    "#### Explaination for RRF (Reciprocal Rank Fusion)\n",
    "- Dense search results: [doc_A, doc_B, doc_C]\n",
    "- Sparse search results: [doc_C, doc_A, doc_D]\n",
    "\n",
    "**RRF scores:**\n",
    "- doc_A: 1/1 (dense) + 1/2 (sparse) = 1.5\n",
    "- doc_B: 1/2 (dense) + 0 (not in sparse) = 0.5\n",
    "- doc_C: 1/3 (dense) + 1/1 (sparse) = 1.33\n",
    "- doc_D: 0 (not in dense) + 1/3 (sparse) = 0.33\n",
    "\n",
    "**Final ranking**: [doc_A, doc_C, doc_B, doc_D]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb22de246f1363",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 7. Reranking with a Cross-Encoder\n",
    "\n",
    "Reranking refines the initial search results using a more accurate model.\n",
    "\n",
    "### 7.1 Loading the Document Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c4bb42afdc331fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:20.126928Z",
     "start_time": "2025-07-28T04:17:19.807677Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since BeIR/scifact couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'corpus' at /Users/sarangsanjaykulkarni/.cache/huggingface/datasets/BeIR___scifact/corpus/0.0.0/984eed826375f18d27936c4a32bf0f8491e3f414 (last modified on Sun Jul  6 21:19:11 2025).\n"
     ]
    }
   ],
   "source": [
    "documents_dataset = load_dataset(\"BeIR/scifact\", \"corpus\", split=\"corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b9754f",
   "metadata": {},
   "source": [
    "### 7.2 Setting Up the Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "787b62b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:24.145931Z",
     "start_time": "2025-07-28T04:17:20.187553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model moved to MPS device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = CrossEncoder('cross-encoder/ms-marco-electra-base')\n",
    "try:\n",
    "    model.model = model.model.to(device)\n",
    "    print(\"Model moved to MPS device\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not move model to MPS device, using CPU: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ac017",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- **Cross-Encoder:** Takes query-document pairs and scores their relevance directly, capturing interactions better than bi-encoders used in initial retrieval.\n",
    "- We optimize for performance by using available hardware (e.g., MPS on Apple Silicon).\n",
    "\n",
    "### 7.3 Reranking Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef65effe22feb082",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:17:24.217112Z",
     "start_time": "2025-07-28T04:17:24.213336Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def rerank(pairs, batch_size=24):  # Adjusted batch size for M3 architecture\n",
    "    # Using batching for more efficient processing\n",
    "    all_scores = []\n",
    "\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch_pairs = pairs[i:i + batch_size]\n",
    "        batch_scores = model.predict(batch_pairs)\n",
    "        all_scores.extend(batch_scores)\n",
    "\n",
    "    return all_scores\n",
    "\n",
    "def process_query(query_id, doc_scores, query_texts, document_texts):\n",
    "    query_text = query_texts.get(query_id, \"\")\n",
    "    query_document_pairs = [(query_text, document_texts.get(doc_id, \"\")) for doc_id in doc_scores.keys()]\n",
    "    scores = rerank(query_document_pairs)\n",
    "    return query_id, {doc_id: score for doc_id, score in zip(doc_scores.keys(), scores)}\n",
    "\n",
    "def reranked_data(data):\n",
    "    # Cache document and query texts to dictionaries to avoid repeated lookups\n",
    "    query_texts = {str(query[\"_id\"]): query[\"text\"] for query in queries_dataset}\n",
    "    document_texts = {str(doc[\"_id\"]): doc[\"text\"] for doc in documents_dataset}\n",
    "\n",
    "    # Determine optimal number of workers based on CPU cores\n",
    "    max_workers = min(os.cpu_count() or 4, 8)  # Limiting to 8 concurrent tasks\n",
    "    print(f\"Using {max_workers} parallel workers\")\n",
    "\n",
    "    results = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_query, query_id, doc_scores, query_texts, document_texts): query_id\n",
    "            for query_id, doc_scores in data.items()\n",
    "        }\n",
    "\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures),\n",
    "                          total=len(futures), desc=\"Reranking queries\"):\n",
    "            query_id = futures[future]\n",
    "            try:\n",
    "                _, updated_scores = future.result()\n",
    "                results[query_id] = updated_scores\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing query {query_id}: {e}\")\n",
    "                # Keep original scores if reranking fails\n",
    "                results[query_id] = data[query_id]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bdfa5d",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- **rerank:** Processes query-document pairs in batches for efficiency.\n",
    "- **process_query:** Pairs a query with its candidate documents and reranks them.\n",
    "- **reranked_data:** Manages parallel processing to rerank all queries’ results.\n",
    "\n",
    "### 7.4 Applying Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "844f21562bebd77f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:18:39.095746Z",
     "start_time": "2025-07-28T04:17:24.262065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting final reranking...\n",
      "Using 8 parallel workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking queries:   0%|          | 0/1109 [00:00<?, ?it/s]/Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `ElectraSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Reranking queries: 100%|██████████| 1109/1109 [01:09<00:00, 15.92it/s]\n"
     ]
    }
   ],
   "source": [
    "query_vectors = {}\n",
    "for query_idx, query in enumerate(queries_dataset):\n",
    "    query_id = str(query[\"_id\"])\n",
    "    query_vectors[query_id] = {\n",
    "        \"dense\": dense_vectors[query_idx],\n",
    "        \"sparse\": sparse_vectors[query_idx]\n",
    "    }\n",
    "\n",
    "# Create prefetch objects for all queries at once\n",
    "prefetch_jobs = []\n",
    "reranker_dict = {}\n",
    "\n",
    "# Process in batches\n",
    "batch_size = 10  # Adjust based on your system's memory\n",
    "for i in range(0, len(query_vectors), batch_size):\n",
    "    batch_queries = {k: query_vectors[k] for k in list(query_vectors.keys())[i:i+batch_size]}\n",
    "\n",
    "    # Execute batch queries\n",
    "    batch_results = {}\n",
    "    for query_id, vectors in batch_queries.items():\n",
    "        prefetch = [\n",
    "            models.Prefetch(\n",
    "                query=vectors[\"dense\"],\n",
    "                using=\"all-MiniLM-L6-v2\",\n",
    "                limit=20,\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.SparseVector(**vectors[\"sparse\"].as_object()),\n",
    "                using=\"bm25\",\n",
    "                limit=20,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        hybrid_search_result = client.query_points(\n",
    "            \"scifact\",\n",
    "            prefetch=prefetch,\n",
    "            query=models.FusionQuery(\n",
    "                fusion=models.Fusion.RRF,\n",
    "            ),\n",
    "            with_payload=False,\n",
    "            limit=5,\n",
    "        )\n",
    "\n",
    "        reranker_dict[query_id] = {\n",
    "            str(point.id): point.score\n",
    "            for point in hybrid_search_result.points\n",
    "        }\n",
    "\n",
    "    # Show progress\n",
    "    # print(f\"Processed queries {i+1} to {min(i+batch_size, len(query_vectors))}\")\n",
    "\n",
    "# Final reranking in one go\n",
    "print(\"Starting final reranking...\")\n",
    "final_data = reranked_data(reranker_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd1b6a",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- We perform hybrid search to get the top 10 candidates, then rerank them with the cross-encoder.\n",
    "- The reranked scores replace the original RRF scores, aiming to improve relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcd55bb0c153fc78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:18:39.158410Z",
     "start_time": "2025-07-28T04:18:39.156485Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'26071782': np.float32(0.0008164996),\n",
       " '29638116': np.float32(2.1777332e-05),\n",
       " '4346436': np.float32(0.00023488396),\n",
       " '10608397': np.float32(0.00070513197),\n",
       " '17388232': np.float32(2.6919299e-05)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb4cc15c804316ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:18:39.236572Z",
     "start_time": "2025-07-28T04:18:39.209131Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision@5': np.float64(0.17082818294190358),\n",
       " 'mrr@5': np.float64(0.6678203543469303)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_rerank_run = Run(final_data, name=\"post-rerank\")\n",
    "evaluate(qrels, post_rerank_run, metrics=[\"precision@5\", \"mrr@5\"], make_comparable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2b8d6",
   "metadata": {},
   "source": [
    "## 8. Comparing All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df2cd830f8583013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:18:39.357258Z",
     "start_time": "2025-07-28T04:18:39.305152Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#    Model            P@5      Recall@5    MRR@5    DCG@5    NDCG@5\n",
       "---  ---------------  -------  ----------  -------  -------  --------\n",
       "a    semantic_search  0.152    0.682       0.576    0.634    0.592\n",
       "b    bm25             0.161    0.736ᵃ      0.647ᵃ   0.701ᵃ   0.665ᵃ\n",
       "c    hybrid_search    0.171ᵃᵇ  0.775ᵃᵇ     0.656ᵃ   0.721ᵃ   0.678ᵃ\n",
       "d    post-rerank      0.171ᵃᵇ  0.774ᵃᵇ     0.668ᵃ   0.730ᵃᵇ  0.687ᵃᵇ"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ranx import compare\n",
    "\n",
    "compare(\n",
    "    qrels=qrels,\n",
    "    runs=[\n",
    "        dense_run,\n",
    "        bm25_run,\n",
    "        rrf_run,\n",
    "        post_rerank_run,\n",
    "    ],\n",
    "    metrics=[\"precision@5\", \"recall@5\", \"mrr@5\", \"dcg@5\", \"ndcg@5\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5794fe3",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- This compares all four methods (dense, sparse, hybrid, and post-rerank) across multiple metrics:\n",
    "  - **Recall@5:** Fraction of relevant documents retrieved in the top 5.\n",
    "  - **DCG@5:** Discounted Cumulative Gain, rewarding higher-ranked relevant documents.\n",
    "  - **NDCG@5:** Normalized DCG, for comparability across queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ca3981da0955dcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:18:42.093361Z",
     "start_time": "2025-07-28T04:18:39.418753Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank(pairs):\n",
    "    model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-4')\n",
    "    return model.predict(pairs)\n",
    "\n",
    "\n",
    "scores = rerank(pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4c3a8a21d67ab1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T04:18:42.157301Z",
     "start_time": "2025-07-28T04:18:42.155341Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.657295e-04, 8.626879e-01], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f329c3091b99fae",
   "metadata": {},
   "source": [
    "## 10. Understanding Evaluation Metrics\n",
    "\n",
    "Here’s what the metrics mean:\n",
    "- **Precision@5:** Fraction of top 5 results that are relevant.\n",
    "- **Recall@5:** Fraction of all relevant documents retrieved in the top 5.\n",
    "- **MRR@5:** Average reciprocal rank of the first relevant result in the top 5.\n",
    "- **DCG@5:** Rewards relevant documents higher up the list.\n",
    "- **NDCG@5:** Normalizes DCG for fair comparison.\n",
    "\n",
    "These metrics show different aspects of search quality, from precision to ranking effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We’ve covered how hybrid search combines dense and sparse methods using RRF, and how reranking with a cross-encoder refines results. These techniques improve search relevance and accuracy by leveraging semantic understanding, keyword matching, and precise relevance scoring. Experiment with different queries or metrics to deepen your understanding!\n",
    "\n",
    "---\n",
    "\n",
    "You can copy each section (markdown and code blocks) into separate cells in a Jupyter notebook. Run the cells in order to set up, test, and experiment with hybrid search and reranking!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
