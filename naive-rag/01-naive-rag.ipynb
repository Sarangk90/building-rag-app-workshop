{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Workshop Notebook - Naive RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:19:59.446024Z",
     "start_time": "2025-03-08T12:19:58.360870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (25.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wikipedia in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: mwparserfromhell in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (0.6.6)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (4.13.3)\n",
      "Requirement already satisfied: openai in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.95.1)\n",
      "Requirement already satisfied: qdrant-client in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.13.2)\n",
      "Requirement already satisfied: tqdm in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dotenv in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from wikipedia) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.7.9)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from beautifulsoup4) (4.14.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (1.73.1)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (1.73.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (2.2.6)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: protobuf<7.0.0,>=6.30.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (6.31.1)\n",
      "Requirement already satisfied: setuptools in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (65.5.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install wikipedia mwparserfromhell beautifulsoup4 openai qdrant-client tqdm python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:19:59.455367Z",
     "start_time": "2025-03-08T12:19:59.451770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion:\n",
    "1. Fetch Wikipedia articles using the Wikipedia API.\n",
    "2. Clean the text by removing wiki markup and citation numbers.\n",
    "3. Chunk the text into smaller pieces to create embeddings.\n",
    "4. Create embeddings using OpenAI's text-embedding-3-small model.\n",
    "5. Index the embeddings using Qdrant Vector Store.\n",
    "\n",
    "![../imgs/ingestion.png](../imgs/ingestion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Fetch Wikipedia articles using the Wikipedia API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.123746Z",
     "start_time": "2025-03-08T12:19:59.463154Z"
    }
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import re\n",
    "from mwparserfromhell import parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "ARTICLE_TITLES = [\n",
    "    \"Deep learning\",\n",
    "    \"Transformer (machine learning model)\",\n",
    "    \"Natural language processing\",\n",
    "    \"Reinforcement learning\",\n",
    "    \"Artificial neural network\",\n",
    "    \"Generative pre-trained transformer\",\n",
    "    \"BERT (language model)\", \"Overfitting\"\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_wikipedia_article(title):\n",
    "    try:\n",
    "        page = wikipedia.page(title)\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"url\": page.url,\n",
    "            \"raw_content\": page.content\n",
    "        }\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return fetch_wikipedia_article(e.options[0])\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"Skipping {title}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Clean the text by removing wiki markup and citation numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Natural language processing\n",
      "Skipping Reinforcement learning\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove wiki markup and citation numbers\n",
    "    text = ''.join(parse(text).strip_code())\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    return re.sub(r'\\[\\d+\\]', '', text).strip()\n",
    "\n",
    "\n",
    "articles = []\n",
    "for title in ARTICLE_TITLES:\n",
    "    article = fetch_wikipedia_article(title)\n",
    "    if article:\n",
    "        article[\"content\"] = clean_text(article[\"raw_content\"])\n",
    "        articles.append(article)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. \\nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.\\n\\nThe modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. Transformers were first developed as an improvement over previou'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1]['content'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Chunk the text into smaller pieces to create embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.149160Z",
     "start_time": "2025-03-08T12:20:14.142830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chunking function\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size])\n",
    "            for i in range(0, len(words), chunk_size - overlap)]\n",
    "\n",
    "\n",
    "# Prepare chunks and metadata\n",
    "corpus = []\n",
    "metadata = []\n",
    "for article in articles:\n",
    "    chunks = chunk_text(article[\"content\"])\n",
    "    corpus.extend(chunks)\n",
    "    metadata.extend([{\"title\": article[\"title\"], \"url\": article[\"url\"]}] * len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.158827Z",
     "start_time": "2025-03-08T12:20:14.156223Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Corpus: 134\n",
      "Total Metadata: 134\n"
     ]
    }
   ],
   "source": [
    "print('Total Corpus:', len(corpus))\n",
    "print('Total Metadata:', len(metadata))\n",
    "\n",
    "deep_learning_chunks = [chunk for chunk, meta in zip(corpus, metadata) if meta['title'] == 'Deep learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.177771Z",
     "start_time": "2025-03-08T12:20:14.175500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deep_learning_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Create embeddings using OpenAI's text-embedding-3-small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.196735Z",
     "start_time": "2025-03-08T12:20:14.190270Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "# Define the embedding function using OpenAI's API (using text-embedding-ada-002)\n",
    "def openai_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=[text],  # Passing the text as a list\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    # Use dot notation to access the embedding from the response object\n",
    "    embeddings = [data.embedding for data in response.data]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.106911Z",
     "start_time": "2025-03-08T12:20:14.201619Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "chunked_texts = []\n",
    "metadata_chunks = []\n",
    "test_corpus = corpus[:10]\n",
    "\n",
    "for chunk in tqdm(test_corpus):\n",
    "    embedding = openai_embedding(chunk)\n",
    "    embeddings.extend(embedding)\n",
    "    chunked_texts.extend([chunk] * len(embedding))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "### 1.5. Index the embeddings using Qdrant Vector Store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.136433Z",
     "start_time": "2025-03-08T12:20:20.121187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "# Create an in-memory Qdrant instance\n",
    "client = QdrantClient(\":memory:\")\n",
    "collection_name = \"wikipedia_articles\"\n",
    "\n",
    "# Create the collection with the specified vector configuration\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Upsert points into the collection using PointStruct for each point\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=idx,\n",
    "            vector=embedding,\n",
    "            payload={\"text\": chunked_texts[idx]}\n",
    "        )\n",
    "        for idx, embedding in enumerate(embeddings)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. Build the Q/A Chatbot\n",
    "\n",
    "![../imgs/naive-rag.png](../imgs/naive-rag.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Retrieval - Search the database for the most relevant embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.471716Z",
     "start_time": "2025-03-08T12:20:20.142631Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'In machine learning, deep learning focuses on utilizing multilayered '\n",
      "         'neural networks to perform tasks such as classification, regression, '\n",
      "         'and representation learning. The field takes inspiration from '\n",
      "         'biological neuroscience and is centered around stacking artificial '\n",
      "         'neurons into layers and \"training\" them to process data. The '\n",
      "         'adjective \"deep\" refers to the use of multiple layers (ranging from '\n",
      "         'three to several hundred or thousands) in the network. Methods used '\n",
      "         'can be supervised, semi-supervised or unsupervised. Some common deep '\n",
      "         'learning network architectures include fully connected networks, '\n",
      "         'deep belief networks, recurrent neural networks, convolutional '\n",
      "         'neural networks, generative adversarial networks, transformers, and '\n",
      "         'neural radiance fields. These architectures have been applied to '\n",
      "         'fields including computer vision, speech recognition, natural '\n",
      "         'language processing, machine translation, bioinformatics, drug '\n",
      "         'design, medical image analysis, climate science, material inspection '\n",
      "         'and board game programs, where they have produced results comparable '\n",
      "         'to and in some cases surpassing human expert performance. Early '\n",
      "         'forms of neural networks were inspired by information processing and '\n",
      "         'distributed communication nodes in biological systems, particularly '\n",
      "         'the human brain. However, current neural networks do not intend to '\n",
      "         'model the brain function of organisms, and are generally seen as '\n",
      "         'low-quality models for that purpose. Overview Most modern deep '\n",
      "         'learning models are based on multi-layered neural networks such as '\n",
      "         'convolutional neural networks and transformers, although they can '\n",
      "         'also include propositional formulas or latent variables organized '\n",
      "         'layer-wise in deep generative models such as the nodes in deep '\n",
      "         'belief networks and deep Boltzmann machines. Fundamentally, deep '\n",
      "         'learning refers to a class of machine learning algorithms in which a '\n",
      "         'hierarchy of layers is used to transform input data into a '\n",
      "         'progressively more abstract and composite representation. For '\n",
      "         'example, in an image recognition model, the raw input may be an '\n",
      "         'image (represented as a tensor of pixels). The first '\n",
      "         'representational layer may attempt to identify basic shapes such as'}\n"
     ]
    }
   ],
   "source": [
    "# Function to search the database\n",
    "def vector_search(query, top_k=3):\n",
    "    # create embedding of the query\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    query_embeddings = response.data[0].embedding\n",
    "    # similarity search using the embedding, give top n results which are close to the query embeddings\n",
    "    search_result = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embeddings,\n",
    "        with_payload=True,\n",
    "        limit=top_k,\n",
    "    ).points\n",
    "    return [result.payload for result in search_result]\n",
    "\n",
    "\n",
    "search_result = vector_search(\"What does the word 'deep' in 'deep learning' refer\")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(search_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generation - Use the retrieved embeddings to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.533382Z",
     "start_time": "2025-03-08T12:20:20.524889Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_generate(prompt, model=\"gpt-4o\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.569179Z",
     "start_time": "2025-03-08T12:20:20.560036Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def prompt_template(question, context):\n",
    "    return \"\"\"You are a AI Assistant that provides answer to the question at the end, over the following\n",
    "  pieces of context. Make sure to only use the context to answer the question. Keep the wording very close to the context\n",
    "  context:\n",
    "  ```\n",
    "  \"\"\" + json.dumps(context) + \"\"\"\n",
    "  ```\n",
    "  User question: \"\"\" + question + \"\"\"\n",
    "  Answer in markdown:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:21.903054Z",
     "start_time": "2025-03-08T12:20:20.582952Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ```markdown\n",
      "The context does not provide specific information about a common evaluation set for image classification.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(question):\n",
    "    #Retrieval: search a knowledge base.\n",
    "    search_result = vector_search(question)\n",
    "\n",
    "    prompt = prompt_template(question, search_result)\n",
    "    # Generation: LLMs' ability to generate the answer\n",
    "    return model_generate(prompt)\n",
    "\n",
    "\n",
    "question = f\"What is A common evaluation set for image classification? \"\n",
    "answer = generate_answer(question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:45:49.559550Z",
     "start_time": "2025-03-08T12:45:46.631155Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ```markdown\n",
      "The time delay neural network (TDNN) was introduced by Alex Waibel in 1987.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "question = f\"Who introduced the time delay neural network (TDNN)? and when ?\"\n",
    "answer = generate_answer(question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:21.936317Z",
     "start_time": "2025-03-08T12:20:21.934810Z"
    }
   },
   "source": [
    "## 3. RAG Evaluation with RAGAS\n",
    "\n",
    "Before proceeding with improvements, let's establish baseline scores using **RAGAS** (Retrieval Augmented Generation Assessment Suite) - a specialized framework designed specifically for evaluating RAG systems.\n",
    "\n",
    "### Context-Focused Metrics:\n",
    "\n",
    "1. **Context Precision**: How well are relevant chunks ranked at the top?\n",
    "2. **Context Recall**: How much of the necessary information was retrieved?\n",
    "3. **Context Relevancy**: How relevant is the retrieved context to the question?\n",
    "\n",
    "We're using **RAGAS** because it's purpose-built for RAG evaluation and provides deep insights into context quality - the most critical component of RAG performance. The evaluation is simple to use - just call one function!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Evaluating your Naive RAG system with RAGAS...\n",
      "This will evaluate context quality metrics on 15 questions...\n",
      "\n",
      "✅ Loaded 14 questions from evaluation dataset\n",
      "\n",
      "Evaluating 14 questions...\n",
      "\n",
      "Question 1/14: Who introduced the ReLU (rectified linear unit) ac...\n",
      "Question 2/14: What was the first working deep learning algorithm...\n",
      "Question 3/14: Which CNN achieved superhuman performance in a vis...\n",
      "Question 4/14: When was BERT introduced and by which organization...\n",
      "Question 5/14: What are the two model sizes BERT was originally i...\n",
      "Question 6/14: What percentage of tokens are randomly selected fo...\n",
      "Question 7/14: Who introduced the term 'deep learning' to the mac...\n",
      "Question 8/14: Which three researchers were awarded the 2018 Turi...\n",
      "Question 9/14: When was the first GPT introduced and by which org...\n",
      "Question 10/14: What were the three parameter sizes of the first v...\n",
      "Question 11/14: What is the 'one in ten rule' in regression analys...\n",
      "Question 12/14: What is the essence of overfitting according to th...\n",
      "Question 13/14: In which year and paper was the modern version of ...\n",
      "Question 14/14: What value did the original Transformer paper use ...\n",
      "\n",
      "🔍 Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f6fe8220434467acafee5f133d30be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAGAS EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "CONTEXT RECALL METRIC (0.0 - 1.0 scale):\n",
      "  🔴 Context Recall: 0.143\n",
      "\n",
      "🔴 POOR: Significant improvements needed in context retrieval.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import the RAGAS evaluation utility\n",
    "from rag_evaluator_v2 import evaluate_naive_rag_v2\n",
    "\n",
    "# Run evaluation on the current RAG system using RAGAS\n",
    "print(\"🔍 Evaluating your Naive RAG system with RAGAS...\")\n",
    "print(\"This will evaluate context quality metrics on 15 questions...\\n\")\n",
    "\n",
    "baseline_results = evaluate_naive_rag_v2(\n",
    "    vector_search_func=vector_search,\n",
    "    generate_answer_func=generate_answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📋 Why We Need These Baseline Scores\n",
    "\n",
    "These **RAGAS-powered** baseline scores are crucial because:\n",
    "\n",
    "1. **Context Quality Focus**: RAGAS specifically measures how well your retrieval system finds and ranks relevant information\n",
    "2. **Purpose-Built for RAG**: Unlike general evaluation tools, RAGAS is designed specifically for RAG systems\n",
    "3. **Objective Measurement**: Quantitative metrics that measure actual retrieval performance\n",
    "4. **Debugging Aid**: Low context scores immediately tell you where your RAG is failing\n",
    "5. **Optimization Guide**: Use these metrics to systematically improve your retrieval strategy\n",
    "\n",
    "🔬 **What makes RAGAS special**: \n",
    "- **Context Precision** helps ensure the most relevant information appears first\n",
    "- **Context Recall** ensures you're not missing important information\n",
    "- **Context Relevancy** validates that retrieved chunks actually help answer the question\n",
    "\n",
    "**Next Steps**: Now that we have our baseline context metrics, let's expand our dataset to demonstrate the limitations of naive RAG approaches!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
