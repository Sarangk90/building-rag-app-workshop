{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Workshop Notebook - Naive RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:19:59.446024Z",
     "start_time": "2025-03-08T12:19:58.360870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (25.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wikipedia in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: mwparserfromhell in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (0.7.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (4.13.4)\n",
      "Requirement already satisfied: openai in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.97.0)\n",
      "Requirement already satisfied: qdrant-client in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.14.3)\n",
      "Requirement already satisfied: tqdm in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dotenv in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from wikipedia) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.7.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from beautifulsoup4) (4.14.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (1.73.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (2.3.1)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from qdrant-client) (6.31.1)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /Users/sarangsanjaykulkarni/projects/personal/building-rag-app-workshop/venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install wikipedia mwparserfromhell beautifulsoup4 openai qdrant-client tqdm python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:19:59.455367Z",
     "start_time": "2025-03-08T12:19:59.451770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion:\n",
    "1. Fetch Wikipedia articles using the Wikipedia API.\n",
    "2. Clean the text by removing wiki markup and citation numbers.\n",
    "3. Chunk the text into smaller pieces to create embeddings.\n",
    "4. Create embeddings using OpenAI's text-embedding-3-small model.\n",
    "5. Index the embeddings using Qdrant Vector Store.\n",
    "\n",
    "![../imgs/ingestion.png](../imgs/ingestion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Fetch Wikipedia articles using the Wikipedia API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.123746Z",
     "start_time": "2025-03-08T12:19:59.463154Z"
    }
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import re\n",
    "from mwparserfromhell import parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "ARTICLE_TITLES = [\n",
    "    \"Deep learning\",\n",
    "    \"Transformer (machine learning model)\",\n",
    "    \"Natural language processing\",\n",
    "    \"Reinforcement learning\",\n",
    "    \"Artificial neural network\",\n",
    "    \"Generative pre-trained transformer\",\n",
    "    \"BERT (language model)\", \"Overfitting\"\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_wikipedia_article(title):\n",
    "    try:\n",
    "        page = wikipedia.page(title)\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"url\": page.url,\n",
    "            \"raw_content\": page.content\n",
    "        }\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return fetch_wikipedia_article(e.options[0])\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"Skipping {title}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Clean the text by removing wiki markup and citation numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Natural language processing\n",
      "Skipping Reinforcement learning\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove wiki markup and citation numbers\n",
    "    text = ''.join(parse(text).strip_code())\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    return re.sub(r'\\[\\d+\\]', '', text).strip()\n",
    "\n",
    "\n",
    "articles = []\n",
    "for title in ARTICLE_TITLES:\n",
    "    article = fetch_wikipedia_article(title)\n",
    "    if article:\n",
    "        article[\"content\"] = clean_text(article[\"raw_content\"])\n",
    "        articles.append(article)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. \\nTransformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLMs) on large (language) datasets.\\n\\nThe modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google. Transformers were first developed as an improvement over previou'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1]['content'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Chunk the text into smaller pieces to create embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.149160Z",
     "start_time": "2025-03-08T12:20:14.142830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chunking function\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size])\n",
    "            for i in range(0, len(words), chunk_size - overlap)]\n",
    "\n",
    "\n",
    "# Prepare chunks and metadata\n",
    "corpus = []\n",
    "metadata = []\n",
    "for article in articles:\n",
    "    chunks = chunk_text(article[\"content\"])\n",
    "    corpus.extend(chunks)\n",
    "    metadata.extend([{\"title\": article[\"title\"], \"url\": article[\"url\"]}] * len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.158827Z",
     "start_time": "2025-03-08T12:20:14.156223Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Corpus: 134\n",
      "Total Metadata: 134\n"
     ]
    }
   ],
   "source": [
    "print('Total Corpus:', len(corpus))\n",
    "print('Total Metadata:', len(metadata))\n",
    "\n",
    "deep_learning_chunks = [chunk for chunk, meta in zip(corpus, metadata) if meta['title'] == 'Deep learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.177771Z",
     "start_time": "2025-03-08T12:20:14.175500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deep_learning_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Create embeddings using OpenAI's text-embedding-3-small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.196735Z",
     "start_time": "2025-03-08T12:20:14.190270Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "# Define the embedding function using OpenAI's API (using text-embedding-ada-002)\n",
    "def openai_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=[text],  # Passing the text as a list\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    # Use dot notation to access the embedding from the response object\n",
    "    embeddings = [data.embedding for data in response.data]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.106911Z",
     "start_time": "2025-03-08T12:20:14.201619Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "chunked_texts = []\n",
    "metadata_chunks = []\n",
    "test_corpus = corpus[:10]\n",
    "\n",
    "for chunk in tqdm(test_corpus):\n",
    "    embedding = openai_embedding(chunk)\n",
    "    embeddings.extend(embedding)\n",
    "    chunked_texts.extend([chunk] * len(embedding))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "### 1.5. Index the embeddings using Qdrant Vector Store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.136433Z",
     "start_time": "2025-03-08T12:20:20.121187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "# Create an in-memory Qdrant instance\n",
    "client = QdrantClient(\":memory:\")\n",
    "collection_name = \"wikipedia_articles\"\n",
    "\n",
    "# Create the collection with the specified vector configuration\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Upsert points into the collection using PointStruct for each point\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=idx,\n",
    "            vector=embedding,\n",
    "            payload={\"text\": chunked_texts[idx]}\n",
    "        )\n",
    "        for idx, embedding in enumerate(embeddings)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. Build the Q/A Chatbot\n",
    "\n",
    "![../imgs/naive-rag.png](../imgs/naive-rag.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Retrieval - Search the database for the most relevant embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.471716Z",
     "start_time": "2025-03-08T12:20:20.142631Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'In machine learning, deep learning focuses on utilizing multilayered '\n",
      "         'neural networks to perform tasks such as classification, regression, '\n",
      "         'and representation learning. The field takes inspiration from '\n",
      "         'biological neuroscience and is centered around stacking artificial '\n",
      "         'neurons into layers and \"training\" them to process data. The '\n",
      "         'adjective \"deep\" refers to the use of multiple layers (ranging from '\n",
      "         'three to several hundred or thousands) in the network. Methods used '\n",
      "         'can be supervised, semi-supervised or unsupervised. Some common deep '\n",
      "         'learning network architectures include fully connected networks, '\n",
      "         'deep belief networks, recurrent neural networks, convolutional '\n",
      "         'neural networks, generative adversarial networks, transformers, and '\n",
      "         'neural radiance fields. These architectures have been applied to '\n",
      "         'fields including computer vision, speech recognition, natural '\n",
      "         'language processing, machine translation, bioinformatics, drug '\n",
      "         'design, medical image analysis, climate science, material inspection '\n",
      "         'and board game programs, where they have produced results comparable '\n",
      "         'to and in some cases surpassing human expert performance. Early '\n",
      "         'forms of neural networks were inspired by information processing and '\n",
      "         'distributed communication nodes in biological systems, particularly '\n",
      "         'the human brain. However, current neural networks do not intend to '\n",
      "         'model the brain function of organisms, and are generally seen as '\n",
      "         'low-quality models for that purpose. Overview Most modern deep '\n",
      "         'learning models are based on multi-layered neural networks such as '\n",
      "         'convolutional neural networks and transformers, although they can '\n",
      "         'also include propositional formulas or latent variables organized '\n",
      "         'layer-wise in deep generative models such as the nodes in deep '\n",
      "         'belief networks and deep Boltzmann machines. Fundamentally, deep '\n",
      "         'learning refers to a class of machine learning algorithms in which a '\n",
      "         'hierarchy of layers is used to transform input data into a '\n",
      "         'progressively more abstract and composite representation. For '\n",
      "         'example, in an image recognition model, the raw input may be an '\n",
      "         'image (represented as a tensor of pixels). The first '\n",
      "         'representational layer may attempt to identify basic shapes such as'}\n"
     ]
    }
   ],
   "source": [
    "# Function to search the database\n",
    "def vector_search(query, top_k=3):\n",
    "    # create embedding of the query\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    query_embeddings = response.data[0].embedding\n",
    "    # similarity search using the embedding, give top n results which are close to the query embeddings\n",
    "    search_result = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embeddings,\n",
    "        with_payload=True,\n",
    "        limit=top_k,\n",
    "    ).points\n",
    "    return [result.payload for result in search_result]\n",
    "\n",
    "\n",
    "search_result = vector_search(\"What does the word 'deep' in 'deep learning' refer\")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(search_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generation - Use the retrieved embeddings to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.533382Z",
     "start_time": "2025-03-08T12:20:20.524889Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_generate(prompt, model=\"gpt-4.1-nano\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.569179Z",
     "start_time": "2025-03-08T12:20:20.560036Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def prompt_template(question, context):\n",
    "    return \"\"\"You are a AI Assistant that provides answer to the question at the end, over the following\n",
    "  pieces of context. Make sure to only use the context to answer the question. Keep the wording very close to the context\n",
    "  context:\n",
    "  ```\n",
    "  \"\"\" + json.dumps(context) + \"\"\"\n",
    "  ```\n",
    "  User question: \"\"\" + question + \"\"\"\n",
    "  Answer in markdown:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:21.903054Z",
     "start_time": "2025-03-08T12:20:20.582952Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ```markdown\n",
      "The context does not provide specific information about a common evaluation set for image classification.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(question):\n",
    "    #Retrieval: search a knowledge base.\n",
    "    search_result = vector_search(question)\n",
    "\n",
    "    prompt = prompt_template(question, search_result)\n",
    "    # Generation: LLMs' ability to generate the answer\n",
    "    return model_generate(prompt)\n",
    "\n",
    "\n",
    "question = f\"What is A common evaluation set for image classification? \"\n",
    "answer = generate_answer(question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:45:49.559550Z",
     "start_time": "2025-03-08T12:45:46.631155Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ```markdown\n",
      "The time delay neural network (TDNN) was introduced by Alex Waibel in 1987.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "question = f\"Who introduced the time delay neural network (TDNN)? and when ?\"\n",
    "answer = generate_answer(question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
