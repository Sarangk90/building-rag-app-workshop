{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Workshop - Naive RAG Challenges\n",
    "\n",
    "This notebook demonstrates the key limitations of naive RAG systems using our extended Wikipedia dataset. We'll focus on scenarios that clearly show where naive RAG fails and why advanced techniques are necessary.\n",
    "\n",
    "## Dataset Overview:\n",
    "\n",
    "- **61 articles** including Wikipedia + long technical blogs from Lilian Weng, arXiv papers\n",
    "- **1,210 pre-chunked** pieces with 300 character chunks, 50 character overlap\n",
    "- **Pre-embedded** using OpenAI text-embedding-3-small\n",
    "- **Cloud-hosted** on Qdrant for reliable access\n",
    "- **Includes cross-domain articles** to demonstrate naive RAG limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Prerequisites\n",
    "\n",
    "## üîó Complete Setup Required\n",
    "\n",
    "Before running this notebook, you **must** complete the workshop setup process. This includes:\n",
    "\n",
    "- Setting up your Qdrant database (Cloud or Docker)\n",
    "- Configuring environment variables\n",
    "- Running the data ingestion script\n",
    "\n",
    "üìñ **Please follow the complete setup guide here: [`SETUP.md`](../SETUP.md)**\n",
    "\n",
    "The setup process takes about 5-10 minutes and only needs to be done once for the entire workshop.\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "- **All workshop notebooks** use the same setup process\n",
    "- You can choose between **Qdrant Cloud** (recommended) or **local Docker**\n",
    "- The setup guide includes comprehensive troubleshooting\n",
    "- Once setup is complete, you can run any workshop notebook\n",
    "\n",
    "**üö´ Do not proceed** with this notebook until you've completed the setup in [`SETUP.md`](../SETUP.md)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Connect to Your Qdrant Cloud Collection\n",
    "\n",
    "Now that you've run the ingestion script, let's connect to your Qdrant collection and verify the data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è  Detected Qdrant Cloud setup\n",
      "‚úÖ Connected to Qdrant Cloud\n",
      "üìö Collection: workshop_wikipedia_extended\n",
      "ü§ñ Embedding model: text-embedding-3-small\n",
      "üåê Qdrant URL: https://193ab6bf-6a0b-4687-9f5a-5c371f663592.eu-west-1-0.aws.cloud.qdrant.io\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Check if required environment variables are set\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate setup\n",
    "if not openai_api_key:\n",
    "    print(\"‚ùå Missing OPENAI_API_KEY environment variable\")\n",
    "    print(\"üí° Please set this in your .env file and restart the notebook\")\n",
    "    raise ValueError(\"OpenAI API key not configured\")\n",
    "\n",
    "if not qdrant_url:\n",
    "    print(\"‚ùå Missing QDRANT_URL environment variable\")\n",
    "    print(\"üí° Please set this in your .env file and restart the notebook\")\n",
    "    raise ValueError(\"Qdrant URL not configured\")\n",
    "\n",
    "# Determine if this is a local or cloud setup\n",
    "is_local_setup = \"localhost\" in qdrant_url.lower()\n",
    "\n",
    "if is_local_setup:\n",
    "    print(\"üê≥ Detected local Docker setup\")\n",
    "    if qdrant_api_key:\n",
    "        print(\"‚ö†Ô∏è  Note: QDRANT_API_KEY not needed for local setup\")\n",
    "else:\n",
    "    print(\"‚òÅÔ∏è  Detected Qdrant Cloud setup\")\n",
    "    if not qdrant_api_key:\n",
    "        print(\"‚ùå Missing QDRANT_API_KEY for cloud setup\")\n",
    "        print(\"üí° Please set this in your .env file and restart the notebook\")\n",
    "        raise ValueError(\"Qdrant API key required for cloud setup\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Initialize Qdrant client (works for both local and cloud)\n",
    "qdrant_client = QdrantClient(\n",
    "    url=qdrant_url,\n",
    "    api_key=qdrant_api_key  # Will be None for local setup, which is fine\n",
    ")\n",
    "\n",
    "# Collection configuration\n",
    "collection_name = \"workshop_wikipedia_extended\"\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "\n",
    "print(f\"‚úÖ Connected to Qdrant {'locally' if is_local_setup else 'Cloud'}\")\n",
    "print(f\"üìö Collection: {collection_name}\")\n",
    "print(f\"ü§ñ Embedding model: {embedding_model}\")\n",
    "print(f\"üåê Qdrant URL: {qdrant_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Verify Collection and Dataset\n",
    "\n",
    "Let's verify that your ingestion was successful and the data is properly loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Using Qdrant Cloud\n",
      "üìä Collection Statistics:\n",
      "   Total chunks: 1,210\n",
      "   Vector dimension: 1536\n",
      "   Distance metric: Cosine\n",
      "‚úÖ Expected number of chunks found! Ingestion was successful.\n",
      "\n",
      "üìù Sample data structure:\n",
      "\n",
      "Chunk 1:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: Bidirectional encoder representations from transformers (BERT) is a language model introduced in Oct...\n",
      "   Chunk 1 of 10\n",
      "\n",
      "Chunk 2:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: Euclidean space. Encoder: a stack of Transformer blocks with self-attention, but without causal mask...\n",
      "   Chunk 2 of 10\n",
      "\n",
      "Chunk 3:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: consists of a sinusoidal function that takes the position in the sequence as input. Segment type: Us...\n",
      "   Chunk 3 of 10\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get collection information\n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    point_count = collection_info.points_count\n",
    "    \n",
    "    print(f\"üîó Using Qdrant {'locally (Docker)' if is_local_setup else 'Cloud'}\")\n",
    "    \n",
    "    if point_count == 0:\n",
    "        print(\"‚ö†Ô∏è Collection exists but is empty!\")\n",
    "        print(\"üí° Please run the ingestion script: python scripts/ingest_to_qdrant_cloud.py\")\n",
    "    else:\n",
    "        print(f\"üìä Collection Statistics:\")\n",
    "        print(f\"   Total chunks: {point_count:,}\")\n",
    "        print(f\"   Vector dimension: {collection_info.config.params.vectors.size}\")\n",
    "        print(f\"   Distance metric: {collection_info.config.params.vectors.distance}\")\n",
    "        \n",
    "        if point_count == 1210:\n",
    "            print(\"‚úÖ Expected number of chunks found! Ingestion was successful.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Expected 1,210 chunks but found {point_count}. Ingestion may be incomplete.\")\n",
    "\n",
    "        # Sample a few points to see the data structure\n",
    "        sample_points = qdrant_client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=3,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )[0]\n",
    "\n",
    "        print(f\"\\nüìù Sample data structure:\")\n",
    "        for i, point in enumerate(sample_points):\n",
    "            payload = point.payload\n",
    "            print(f\"\\nChunk {i+1}:\")\n",
    "            print(f\"   Title: {payload.get('title', 'Unknown')}\")\n",
    "            print(f\"   Text preview: {payload.get('text', '')[:100]}...\")\n",
    "            print(f\"   Chunk {payload.get('chunk_index', 0)+1} of {payload.get('total_chunks', 0)}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing collection '{collection_name}': {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"1. Make sure you've run: python scripts/ingest_to_qdrant_cloud.py\")\n",
    "    if is_local_setup:\n",
    "        print(\"2. For Docker setup: Check if container is running with 'docker ps'\")\n",
    "        print(\"3. Restart Qdrant if needed: docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant:v1.13.2\")\n",
    "    else:\n",
    "        print(\"2. Check your QDRANT_URL and QDRANT_API_KEY in .env file\")\n",
    "        print(\"3. Verify your Qdrant Cloud cluster is running\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîß Troubleshooting Tips\n",
    "\n",
    "If you're having issues with the setup, here are common solutions:\n",
    "\n",
    "### Collection Not Found Error\n",
    "```bash\n",
    "# Make sure you're in the project root directory\n",
    "cd path/to/building-rag-app-workshop\n",
    "\n",
    "# Run the ingestion script\n",
    "python scripts/ingest_to_qdrant_cloud.py\n",
    "```\n",
    "\n",
    "### Docker Setup Issues (Option B)\n",
    "```bash\n",
    "# Check if Qdrant container is running\n",
    "docker ps\n",
    "\n",
    "# If not running, start it again\n",
    "docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant:v1.13.2\n",
    "\n",
    "# Test connection\n",
    "curl http://localhost:6333\n",
    "```\n",
    "\n",
    "### Cloud Setup Issues (Option A)\n",
    "- Verify your Qdrant Cloud cluster is running in the dashboard\n",
    "- Double-check your cluster URL and API key\n",
    "- Make sure you're using the correct cluster region\n",
    "\n",
    "### Environment Variables Issues\n",
    "- Double-check your `.env` file is in the project root\n",
    "- Restart your Jupyter kernel after creating/updating `.env`\n",
    "- For local setup: `QDRANT_URL=http://localhost:6333` (no API key needed)\n",
    "- For cloud setup: Both `QDRANT_URL` and `QDRANT_API_KEY` required\n",
    "\n",
    "### OpenAI API Issues\n",
    "- Make sure you have credits in your OpenAI account\n",
    "- Verify your OpenAI API key is correct\n",
    "\n",
    "### Still Having Issues?\n",
    "- Check the `data/ingestion_summary.json` file (created after successful ingestion)\n",
    "- Look at the terminal output from the ingestion script for error messages\n",
    "- For Docker: Check Docker logs with `docker logs <container_id>`\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ Once you see \"Expected number of chunks found! Ingestion was successful.\" above, you're ready to continue!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Q/A Chatbot\n",
    "\n",
    "Now we can focus on the core RAG functionality without worrying about data preparation!\n",
    "\n",
    "![../imgs/naive-rag.png](../imgs/naive-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Retrieval - Search the cloud database for relevant embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query, top_k=2):\n",
    "    \"\"\"Search the Qdrant Cloud collection for relevant chunks.\"\"\"\n",
    "    # Create embedding of the query\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=query,\n",
    "        model=embedding_model\n",
    "    )\n",
    "    query_embeddings = response.data[0].embedding\n",
    "    \n",
    "    # Similarity search using the embedding\n",
    "    search_result = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embeddings,\n",
    "        with_payload=True,\n",
    "        limit=top_k,\n",
    "    ).points\n",
    "    \n",
    "    return [result.payload for result in search_result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generation - Use retrieved chunks to generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def model_generate(prompt, model=\"gpt-4o\"):\n",
    "    \"\"\"Generate response using OpenAI's chat completion.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # Deterministic output\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def prompt_template(question, context):\n",
    "    \"\"\"Create a prompt template for RAG.\"\"\"\n",
    "    return f\"\"\"You are an AI Assistant that provides answers to questions based on the following context. \n",
    "Make sure to only use the context to answer the question. Keep the wording very close to the context.\n",
    "\n",
    "Context:\n",
    "```\n",
    "{json.dumps(context)}\n",
    "```\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "Answer in markdown:\"\"\"\n",
    "\n",
    "def generate_answer(question):\n",
    "    \"\"\"Complete RAG pipeline: retrieve and generate.\"\"\"\n",
    "    # Retrieval: search the knowledge base\n",
    "    search_result = vector_search(question)\n",
    "    if not search_result:\n",
    "        return \"No relevant information found.\"\n",
    "        \n",
    "    \n",
    "    # Generation: create prompt and generate answer\n",
    "    prompt = prompt_template(question, search_result)\n",
    "    return model_generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Test Basic RAG Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Question: What does the word 'deep' in 'deep learning' refer to?\n",
      "\n",
      "üìö Retrieved Sources:\n",
      "\n",
      "ü§ñ Generated Answer:\n",
      "The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output, describing potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.\n"
     ]
    }
   ],
   "source": [
    "# Test with a clear, unambiguous question first\n",
    "question = \"What does the word 'deep' in 'deep learning' refer to?\"\n",
    "search_result = vector_search(question, top_k=3)\n",
    "\n",
    "print(f\"üîç Question: {question}\")\n",
    "print(f\"\\nüìö Retrieved Sources:\")\n",
    "\n",
    "# Generate answer\n",
    "answer = generate_answer(question)\n",
    "print(f\"\\nü§ñ Generated Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Evaluation with RAGAS\n",
    "\n",
    "Now let's evaluate our naive RAG system using **RAGAS** to establish baseline performance metrics and quantify the confusion we've observed.\n",
    "\n",
    "### Context-Focused Metrics:\n",
    "\n",
    "1. **Context Precision**: How well are relevant chunks ranked at the top?\n",
    "2. **Context Recall**: How much of the necessary information was retrieved?\n",
    "3. **Context Relevancy**: How relevant is the retrieved context to the question?\n",
    "\n",
    "We're using **RAGAS** because it's purpose-built for RAG evaluation and provides deep insights into context quality - the most critical component of RAG performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Evaluating your Naive RAG system with RAGAS...\n",
      "This will evaluate context quality metrics on 15 questions...\n",
      "\n",
      "‚úÖ Loaded 14 questions from evaluation dataset\n",
      "\n",
      "Evaluating 14 questions...\n",
      "\n",
      "Question 1/14: Who introduced the ReLU (rectified linear unit) ac...\n",
      "Question 2/14: What was the first working deep learning algorithm...\n",
      "Question 3/14: Which CNN achieved superhuman performance in a vis...\n",
      "Question 4/14: When was BERT introduced and by which organization...\n",
      "Question 5/14: What are the two model sizes BERT was originally i...\n",
      "Question 6/14: What percentage of tokens are randomly selected fo...\n",
      "Question 7/14: Who introduced the term 'deep learning' to the mac...\n",
      "Question 8/14: Which three researchers were awarded the 2018 Turi...\n",
      "Question 9/14: When was the first GPT introduced and by which org...\n",
      "Question 10/14: What were the three parameter sizes of the first v...\n",
      "Question 11/14: What is the 'one in ten rule' in regression analys...\n",
      "Question 12/14: What is the essence of overfitting according to th...\n",
      "Question 13/14: In which year and paper was the modern version of ...\n",
      "Question 14/14: What value did the original Transformer paper use ...\n",
      "\n",
      "üîç Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1255a25896b74df692e5dcc0dbe749fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAGAS EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üìã INDIVIDUAL QUESTION SCORES:\n",
      "------------------------------------------------------------\n",
      " 1. üü¢ 1.000 - Who introduced the ReLU (rectified linear unit) activation f...\n",
      " 2. üü¢ 1.000 - What was the first working deep learning algorithm and who p...\n",
      " 3. üü¢ 1.000 - Which CNN achieved superhuman performance in a visual patter...\n",
      " 4. üî¥ 0.000 - When was BERT introduced and by which organization?\n",
      " 5. üü¢ 1.000 - What are the two model sizes BERT was originally implemented...\n",
      " 6. üü¢ 1.000 - What percentage of tokens are randomly selected for the mask...\n",
      " 7. üî¥ 0.000 - Who introduced the term 'deep learning' to the machine learn...\n",
      " 8. üî¥ 0.000 - Which three researchers were awarded the 2018 Turing Award f...\n",
      " 9. üü¢ 1.000 - When was the first GPT introduced and by which organization?\n",
      "10. üü¢ 1.000 - What were the three parameter sizes of the first versions of...\n",
      "11. üü¢ 1.000 - What is the 'one in ten rule' in regression analysis?\n",
      "12. üü¢ 1.000 - What is the essence of overfitting according to the article?\n",
      "13. üü¢ 1.000 - In which year and paper was the modern version of the transf...\n",
      "14. üü¢ 1.000 - What value did the original Transformer paper use for the pa...\n",
      "\n",
      "============================================================\n",
      "üìä AGGREGATE RESULTS\n",
      "============================================================\n",
      "\n",
      "CONTEXT RECALL METRIC (0.0 - 1.0 scale):\n",
      "  üü° Context Recall: 0.786\n",
      "============================================================\n",
      "\n",
      "üí° Tip: Add show_detailed=True to see full question details\n"
     ]
    }
   ],
   "source": [
    "# Import the RAGAS evaluation utility\n",
    "from rag_evaluator_v2 import evaluate_naive_rag_v2\n",
    "\n",
    "# Run evaluation on the current RAG system using RAGAS\n",
    "print(\"üîç Evaluating your Naive RAG system with RAGAS...\")\n",
    "print(\"This will evaluate context quality metrics on 15 questions...\\n\")\n",
    "\n",
    "baseline_results = evaluate_naive_rag_v2(\n",
    "    vector_search_func=vector_search,\n",
    "    generate_answer_func=generate_answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
