{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Workshop - Naive RAG Challenges\n",
    "\n",
    "This notebook demonstrates the key limitations of naive RAG systems using our extended Wikipedia dataset. We'll focus on scenarios that clearly show where naive RAG fails and why advanced techniques are necessary.\n",
    "\n",
    "## Dataset Overview:\n",
    "\n",
    "- **61 articles** including Wikipedia + long technical blogs from Lilian Weng, arXiv papers\n",
    "- **1,210 pre-chunked** pieces with 300 character chunks, 50 character overlap\n",
    "- **Pre-embedded** using OpenAI text-embedding-3-small\n",
    "- **Cloud-hosted** on Qdrant for reliable access\n",
    "- **Includes cross-domain articles** to demonstrate naive RAG limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Your Own Qdrant Cloud Collection\n",
    "\n",
    "For this workshop, you'll need to set up your own Qdrant Cloud instance and ingest the extended Wikipedia dataset. Follow these steps:\n",
    "\n",
    "## üöÄ Prerequisites Setup\n",
    "\n",
    "### Step 1: Create Qdrant Cloud Account\n",
    "1. Go to [Qdrant Cloud](https://cloud.qdrant.io/)\n",
    "2. Sign up for a free account\n",
    "3. Create a new cluster (free tier is sufficient)\n",
    "4. Get your cluster URL and API key from the dashboard\n",
    "\n",
    "### Step 2: Set Environment Variables\n",
    "Create a `.env` file in the project root with:\n",
    "```bash\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "QDRANT_URL=your_qdrant_cluster_url_here\n",
    "QDRANT_API_KEY=your_qdrant_api_key_here\n",
    "```\n",
    "\n",
    "### Step 3: Run Data Ingestion Script\n",
    "Before running this notebook, execute the ingestion script to populate your Qdrant collection:\n",
    "\n",
    "```bash\n",
    "# From the project root directory\n",
    "python scripts/ingest_to_qdrant_cloud.py\n",
    "```\n",
    "\n",
    "This script will:\n",
    "- ‚úÖ Load the extended Wikipedia dataset (61 articles)\n",
    "- üî™ Create 1,210 chunks with 300 character chunks, 50 character overlap  \n",
    "- ü§ñ Generate embeddings using OpenAI text-embedding-3-small\n",
    "- üì§ Upload everything to your Qdrant Cloud collection\n",
    "- ‚è±Ô∏è Takes approximately 5-10 minutes to complete\n",
    "\n",
    "**‚ö†Ô∏è Important**: Make sure you have run the ingestion script successfully before proceeding with this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Connect to Your Qdrant Cloud Collection\n",
    "\n",
    "Now that you've run the ingestion script, let's connect to your Qdrant collection and verify the data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Qdrant Cloud\n",
      "üìö Collection: workshop_wikipedia_extended\n",
      "ü§ñ Embedding model: text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Verify environment variables are set\n",
    "required_vars = [\"OPENAI_API_KEY\", \"QDRANT_URL\", \"QDRANT_API_KEY\"]\n",
    "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"‚ùå Missing required environment variables:\")\n",
    "    for var in missing_vars:\n",
    "        print(f\"   - {var}\")\n",
    "    print(\"\\nüí° Please set these in your .env file and restart the notebook\")\n",
    "    raise ValueError(\"Environment variables not configured\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Initialize Qdrant Cloud client\n",
    "qdrant_client = QdrantClient(\n",
    "    url=os.getenv(\"QDRANT_URL\"),\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    ")\n",
    "\n",
    "# Collection configuration\n",
    "collection_name = \"workshop_wikipedia_extended\"\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "\n",
    "print(f\"‚úÖ Connected to Qdrant Cloud\")\n",
    "print(f\"üìö Collection: {collection_name}\")\n",
    "print(f\"ü§ñ Embedding model: {embedding_model}\")\n",
    "print(f\"üåê Qdrant URL: {os.getenv('QDRANT_URL')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Verify Collection and Dataset\n",
    "\n",
    "Let's verify that your ingestion was successful and the data is properly loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Collection Statistics:\n",
      "   Total chunks: 1,210\n",
      "   Vector dimension: 1536\n",
      "   Distance metric: Cosine\n",
      "\n",
      "üìù Sample data structure:\n",
      "\n",
      "Chunk 1:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: Bidirectional encoder representations from transformers (BERT) is a language model introduced in Oct...\n",
      "   Chunk 1 of 10\n",
      "\n",
      "Chunk 2:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: Euclidean space. Encoder: a stack of Transformer blocks with self-attention, but without causal mask...\n",
      "   Chunk 2 of 10\n",
      "\n",
      "Chunk 3:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: consists of a sinusoidal function that takes the position in the sequence as input. Segment type: Us...\n",
      "   Chunk 3 of 10\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get collection information\n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    point_count = collection_info.points_count\n",
    "    \n",
    "    if point_count == 0:\n",
    "        print(\"‚ö†Ô∏è Collection exists but is empty!\")\n",
    "        print(\"üí° Please run the ingestion script: python scripts/ingest_to_qdrant_cloud.py\")\n",
    "    else:\n",
    "        print(f\"üìä Collection Statistics:\")\n",
    "        print(f\"   Total chunks: {point_count:,}\")\n",
    "        print(f\"   Vector dimension: {collection_info.config.params.vectors.size}\")\n",
    "        print(f\"   Distance metric: {collection_info.config.params.vectors.distance}\")\n",
    "        \n",
    "        if point_count == 1210:\n",
    "            print(\"‚úÖ Expected number of chunks found! Ingestion was successful.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Expected 1,210 chunks but found {point_count}. Ingestion may be incomplete.\")\n",
    "\n",
    "        # Sample a few points to see the data structure\n",
    "        sample_points = qdrant_client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=3,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )[0]\n",
    "\n",
    "        print(f\"\\nüìù Sample data structure:\")\n",
    "        for i, point in enumerate(sample_points):\n",
    "            payload = point.payload\n",
    "            print(f\"\\nChunk {i+1}:\")\n",
    "            print(f\"   Title: {payload.get('title', 'Unknown')}\")\n",
    "            print(f\"   Text preview: {payload.get('text', '')[:100]}...\")\n",
    "            print(f\"   Chunk {payload.get('chunk_index', 0)+1} of {payload.get('total_chunks', 0)}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing collection '{collection_name}': {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"1. Make sure you've run: python scripts/ingest_to_qdrant_cloud.py\")\n",
    "    print(\"2. Check your QDRANT_URL and QDRANT_API_KEY in .env file\")\n",
    "    print(\"3. Verify your Qdrant Cloud cluster is running\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîß Troubleshooting Tips\n",
    "\n",
    "If you're having issues with the setup, here are common solutions:\n",
    "\n",
    "### Collection Not Found Error\n",
    "```bash\n",
    "# Make sure you're in the project root directory\n",
    "cd path/to/building-rag-app-workshop\n",
    "\n",
    "# Run the ingestion script\n",
    "python scripts/ingest_to_qdrant_cloud.py\n",
    "```\n",
    "\n",
    "### Environment Variables Issues\n",
    "- Double-check your `.env` file is in the project root\n",
    "- Restart your Jupyter kernel after creating/updating `.env`\n",
    "- Verify your Qdrant Cloud cluster URL and API key\n",
    "\n",
    "### OpenAI API Issues\n",
    "- Make sure you have credits in your OpenAI account\n",
    "- Verify your OpenAI API key is correct\n",
    "\n",
    "### Still Having Issues?\n",
    "- Check the `data/ingestion_summary.json` file (created after successful ingestion)\n",
    "- Look at the terminal output from the ingestion script for error messages\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ Once you see \"Expected number of chunks found! Ingestion was successful.\" above, you're ready to continue!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Q/A Chatbot\n",
    "\n",
    "Now we can focus on the core RAG functionality without worrying about data preparation!\n",
    "\n",
    "![../imgs/naive-rag.png](../imgs/naive-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Retrieval - Search the cloud database for relevant embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query, top_k=2):\n",
    "    \"\"\"Search the Qdrant Cloud collection for relevant chunks.\"\"\"\n",
    "    # Create embedding of the query\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=query,\n",
    "        model=embedding_model\n",
    "    )\n",
    "    query_embeddings = response.data[0].embedding\n",
    "    \n",
    "    # Similarity search using the embedding\n",
    "    search_result = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embeddings,\n",
    "        with_payload=True,\n",
    "        limit=top_k,\n",
    "    ).points\n",
    "    \n",
    "    return [result.payload for result in search_result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generation - Use retrieved chunks to generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def model_generate(prompt, model=\"gpt-4o\"):\n",
    "    \"\"\"Generate response using OpenAI's chat completion.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # Deterministic output\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def prompt_template(question, context):\n",
    "    \"\"\"Create a prompt template for RAG.\"\"\"\n",
    "    return f\"\"\"You are an AI Assistant that provides answers to questions based on the following context. \n",
    "Make sure to only use the context to answer the question. Keep the wording very close to the context.\n",
    "\n",
    "Context:\n",
    "```\n",
    "{json.dumps(context)}\n",
    "```\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "Answer in markdown:\"\"\"\n",
    "\n",
    "def generate_answer(question):\n",
    "    \"\"\"Complete RAG pipeline: retrieve and generate.\"\"\"\n",
    "    # Retrieval: search the knowledge base\n",
    "    search_result = vector_search(question)\n",
    "    if not search_result:\n",
    "        return \"No relevant information found.\"\n",
    "        \n",
    "    \n",
    "    # Generation: create prompt and generate answer\n",
    "    prompt = prompt_template(question, search_result)\n",
    "    return model_generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Test Basic RAG Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Question: What does the word 'deep' in 'deep learning' refer to?\n",
      "\n",
      "üìö Retrieved Sources:\n",
      "\n",
      "ü§ñ Generated Answer:\n",
      "The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output, describing potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.\n"
     ]
    }
   ],
   "source": [
    "# Test with a clear, unambiguous question first\n",
    "question = \"What does the word 'deep' in 'deep learning' refer to?\"\n",
    "search_result = vector_search(question, top_k=3)\n",
    "\n",
    "print(f\"üîç Question: {question}\")\n",
    "print(f\"\\nüìö Retrieved Sources:\")\n",
    "\n",
    "# Generate answer\n",
    "answer = generate_answer(question)\n",
    "print(f\"\\nü§ñ Generated Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Evaluation with RAGAS\n",
    "\n",
    "Now let's evaluate our naive RAG system using **RAGAS** to establish baseline performance metrics and quantify the confusion we've observed.\n",
    "\n",
    "### Context-Focused Metrics:\n",
    "\n",
    "1. **Context Precision**: How well are relevant chunks ranked at the top?\n",
    "2. **Context Recall**: How much of the necessary information was retrieved?\n",
    "3. **Context Relevancy**: How relevant is the retrieved context to the question?\n",
    "\n",
    "We're using **RAGAS** because it's purpose-built for RAG evaluation and provides deep insights into context quality - the most critical component of RAG performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Evaluating your Naive RAG system with RAGAS...\n",
      "This will evaluate context quality metrics on 15 questions...\n",
      "\n",
      "‚úÖ Loaded 14 questions from evaluation dataset\n",
      "\n",
      "Evaluating 14 questions...\n",
      "\n",
      "Question 1/14: Who introduced the ReLU (rectified linear unit) ac...\n",
      "Question 2/14: What was the first working deep learning algorithm...\n",
      "Question 3/14: Which CNN achieved superhuman performance in a vis...\n",
      "Question 4/14: When was BERT introduced and by which organization...\n",
      "Question 5/14: What are the two model sizes BERT was originally i...\n",
      "Question 6/14: What percentage of tokens are randomly selected fo...\n",
      "Question 7/14: Who introduced the term 'deep learning' to the mac...\n",
      "Question 8/14: Which three researchers were awarded the 2018 Turi...\n",
      "Question 9/14: When was the first GPT introduced and by which org...\n",
      "Question 10/14: What were the three parameter sizes of the first v...\n",
      "Question 11/14: What is the 'one in ten rule' in regression analys...\n",
      "Question 12/14: What is the essence of overfitting according to th...\n",
      "Question 13/14: In which year and paper was the modern version of ...\n",
      "Question 14/14: What value did the original Transformer paper use ...\n",
      "\n",
      "üîç Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4a05c1e36f4f03a555944e6319ecc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAGAS EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üìã INDIVIDUAL QUESTION SCORES:\n",
      "------------------------------------------------------------\n",
      " 1. üü¢ 1.000 - Who introduced the ReLU (rectified linear unit) activation f...\n",
      " 2. üü¢ 1.000 - What was the first working deep learning algorithm and who p...\n",
      " 3. üü¢ 1.000 - Which CNN achieved superhuman performance in a visual patter...\n",
      " 4. üî¥ 0.000 - When was BERT introduced and by which organization?\n",
      " 5. üü¢ 1.000 - What are the two model sizes BERT was originally implemented...\n",
      " 6. üü¢ 1.000 - What percentage of tokens are randomly selected for the mask...\n",
      " 7. üî¥ 0.000 - Who introduced the term 'deep learning' to the machine learn...\n",
      " 8. üî¥ 0.000 - Which three researchers were awarded the 2018 Turing Award f...\n",
      " 9. üü¢ 1.000 - When was the first GPT introduced and by which organization?\n",
      "10. üü¢ 1.000 - What were the three parameter sizes of the first versions of...\n",
      "11. üü¢ 1.000 - What is the 'one in ten rule' in regression analysis?\n",
      "12. üü¢ 1.000 - What is the essence of overfitting according to the article?\n",
      "13. üü¢ 1.000 - In which year and paper was the modern version of the transf...\n",
      "14. üü¢ 1.000 - What value did the original Transformer paper use for the pa...\n",
      "\n",
      "============================================================\n",
      "üìä AGGREGATE RESULTS\n",
      "============================================================\n",
      "\n",
      "CONTEXT RECALL METRIC (0.0 - 1.0 scale):\n",
      "  üü° Context Recall: 0.786\n",
      "============================================================\n",
      "\n",
      "üí° Tip: Add show_detailed=True to see full question details\n"
     ]
    }
   ],
   "source": [
    "# Import the RAGAS evaluation utility\n",
    "from rag_evaluator_v2 import evaluate_naive_rag_v2\n",
    "\n",
    "# Run evaluation on the current RAG system using RAGAS\n",
    "print(\"üîç Evaluating your Naive RAG system with RAGAS...\")\n",
    "print(\"This will evaluate context quality metrics on 15 questions...\\n\")\n",
    "\n",
    "baseline_results = evaluate_naive_rag_v2(\n",
    "    vector_search_func=vector_search,\n",
    "    generate_answer_func=generate_answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
