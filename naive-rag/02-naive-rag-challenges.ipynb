{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Workshop - Naive RAG Challenges\n",
    "# Run 'uv sync' in the project root if dependencies are missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Workshop - Naive RAG Challenges\n",
    "\n",
    "This notebook demonstrates the key limitations of naive RAG systems using our extended Wikipedia dataset. We'll focus on scenarios that clearly show where naive RAG fails and why advanced techniques are necessary.\n",
    "\n",
    "## Dataset Overview:\n",
    "\n",
    "- **61 articles** including Wikipedia + long technical blogs from Lilian Weng, arXiv papers\n",
    "- **1,210 pre-chunked** pieces with 300 character chunks, 50 character overlap\n",
    "- **Pre-embedded** using OpenAI text-embedding-3-small\n",
    "- **Cloud-hosted** on Qdrant for reliable access\n",
    "- **Includes cross-domain articles** to demonstrate naive RAG limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Prerequisites\n",
    "\n",
    "## üîó Complete Setup Required\n",
    "\n",
    "Before running this notebook, you **must** complete the workshop setup process. This includes:\n",
    "\n",
    "- Setting up your Qdrant database (Cloud or Docker)\n",
    "- Configuring environment variables\n",
    "- Running the data ingestion script\n",
    "\n",
    "üìñ **Please follow the complete setup guide here: [`SETUP.md`](../SETUP.md)**\n",
    "\n",
    "The setup process takes about 5-10 minutes and only needs to be done once for the entire workshop.\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "- **All workshop notebooks** use the same setup process\n",
    "- You can choose between **Qdrant Cloud** (recommended) or **local Docker**\n",
    "- The setup guide includes comprehensive troubleshooting\n",
    "- Once setup is complete, you can run any workshop notebook\n",
    "\n",
    "**üö´ Do not proceed** with this notebook until you've completed the setup in [`SETUP.md`](../SETUP.md)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T15:45:34.148139Z",
     "start_time": "2025-09-11T15:45:34.141888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Connect to Your Qdrant Cloud Collection\n",
    "\n",
    "Now that you've run the ingestion script, let's connect to your Qdrant collection and verify the data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T15:45:36.204571Z",
     "start_time": "2025-09-11T15:45:34.168372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è  Detected Qdrant Cloud setup\n",
      "‚úÖ Connected to Qdrant Cloud\n",
      "üìö Collection: workshop_wikipedia_extended\n",
      "ü§ñ Embedding model: text-embedding-3-small\n",
      "üåê Qdrant URL: https://18cd8b2c-252d-4e81-8824-dbfbb22674f6.europe-west3-0.gcp.cloud.qdrant.io:6333\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Check if required environment variables are set\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate setup\n",
    "if not openai_api_key:\n",
    "    print(\"‚ùå Missing OPENAI_API_KEY environment variable\")\n",
    "    print(\"üí° Please set this in your .env file and restart the notebook\")\n",
    "    raise ValueError(\"OpenAI API key not configured\")\n",
    "\n",
    "if not qdrant_url:\n",
    "    print(\"‚ùå Missing QDRANT_URL environment variable\")\n",
    "    print(\"üí° Please set this in your .env file and restart the notebook\")\n",
    "    raise ValueError(\"Qdrant URL not configured\")\n",
    "\n",
    "# Determine if this is a local or cloud setup\n",
    "is_local_setup = \"localhost\" in qdrant_url.lower()\n",
    "\n",
    "if is_local_setup:\n",
    "    print(\"üê≥ Detected local Docker setup\")\n",
    "    if qdrant_api_key:\n",
    "        print(\"‚ö†Ô∏è  Note: QDRANT_API_KEY not needed for local setup\")\n",
    "else:\n",
    "    print(\"‚òÅÔ∏è  Detected Qdrant Cloud setup\")\n",
    "    if not qdrant_api_key:\n",
    "        print(\"‚ùå Missing QDRANT_API_KEY for cloud setup\")\n",
    "        print(\"üí° Please set this in your .env file and restart the notebook\")\n",
    "        raise ValueError(\"Qdrant API key required for cloud setup\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Initialize Qdrant client (works for both local and cloud)\n",
    "qdrant_client = QdrantClient(\n",
    "    url=qdrant_url,\n",
    "    api_key=qdrant_api_key  # Will be None for local setup, which is fine\n",
    ")\n",
    "\n",
    "# Collection configuration\n",
    "collection_name = \"workshop_wikipedia_extended\"\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "\n",
    "print(f\"‚úÖ Connected to Qdrant {'locally' if is_local_setup else 'Cloud'}\")\n",
    "print(f\"üìö Collection: {collection_name}\")\n",
    "print(f\"ü§ñ Embedding model: {embedding_model}\")\n",
    "print(f\"üåê Qdrant URL: {qdrant_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Verify Collection and Dataset\n",
    "\n",
    "Let's verify that your ingestion was successful and the data is properly loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T15:45:37.330684Z",
     "start_time": "2025-09-11T15:45:36.213284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Using Qdrant Cloud\n",
      "üìä Collection Statistics:\n",
      "   Total chunks: 1,210\n",
      "   Vector dimension: 1536\n",
      "   Distance metric: Cosine\n",
      "‚úÖ Expected number of chunks found! Ingestion was successful.\n",
      "\n",
      "üìù Sample data structure:\n",
      "\n",
      "Chunk 1:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: Bidirectional encoder representations from transformers (BERT) is a language model introduced in Oct...\n",
      "   Chunk 1 of 10\n",
      "\n",
      "Chunk 2:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: Euclidean space. Encoder: a stack of Transformer blocks with self-attention, but without causal mask...\n",
      "   Chunk 2 of 10\n",
      "\n",
      "Chunk 3:\n",
      "   Title: BERT (language model)\n",
      "   Text preview: consists of a sinusoidal function that takes the position in the sequence as input. Segment type: Us...\n",
      "   Chunk 3 of 10\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get collection information\n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    point_count = collection_info.points_count\n",
    "    \n",
    "    print(f\"üîó Using Qdrant {'locally (Docker)' if is_local_setup else 'Cloud'}\")\n",
    "    \n",
    "    if point_count == 0:\n",
    "        print(\"‚ö†Ô∏è Collection exists but is empty!\")\n",
    "        print(\"üí° Please run the ingestion script: python scripts/ingest_to_qdrant_cloud.py\")\n",
    "    else:\n",
    "        print(f\"üìä Collection Statistics:\")\n",
    "        print(f\"   Total chunks: {point_count:,}\")\n",
    "        print(f\"   Vector dimension: {collection_info.config.params.vectors.size}\")\n",
    "        print(f\"   Distance metric: {collection_info.config.params.vectors.distance}\")\n",
    "        \n",
    "        if point_count == 1210:\n",
    "            print(\"‚úÖ Expected number of chunks found! Ingestion was successful.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Expected 1,210 chunks but found {point_count}. Ingestion may be incomplete.\")\n",
    "\n",
    "        # Sample a few points to see the data structure\n",
    "        sample_points = qdrant_client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=3,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )[0]\n",
    "\n",
    "        print(f\"\\nüìù Sample data structure:\")\n",
    "        for i, point in enumerate(sample_points):\n",
    "            payload = point.payload\n",
    "            print(f\"\\nChunk {i+1}:\")\n",
    "            print(f\"   Title: {payload.get('title', 'Unknown')}\")\n",
    "            print(f\"   Text preview: {payload.get('text', '')[:100]}...\")\n",
    "            print(f\"   Chunk {payload.get('chunk_index', 0)+1} of {payload.get('total_chunks', 0)}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing collection '{collection_name}': {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"1. Make sure you've run: python scripts/ingest_to_qdrant_cloud.py\")\n",
    "    if is_local_setup:\n",
    "        print(\"2. For Docker setup: Check if container is running with 'docker ps'\")\n",
    "        print(\"3. Restart Qdrant if needed: docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant:v1.13.2\")\n",
    "    else:\n",
    "        print(\"2. Check your QDRANT_URL and QDRANT_API_KEY in .env file\")\n",
    "        print(\"3. Verify your Qdrant Cloud cluster is running\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîß Troubleshooting Tips\n",
    "\n",
    "If you're having issues with the setup, here are common solutions:\n",
    "\n",
    "### Collection Not Found Error\n",
    "```bash\n",
    "# Make sure you're in the project root directory\n",
    "cd path/to/building-rag-app-workshop\n",
    "\n",
    "# Run the ingestion script\n",
    "python scripts/ingest_to_qdrant_cloud.py\n",
    "```\n",
    "\n",
    "### Docker Setup Issues (Option B)\n",
    "```bash\n",
    "# Check if Qdrant container is running\n",
    "docker ps\n",
    "\n",
    "# If not running, start it again\n",
    "docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant:v1.13.2\n",
    "\n",
    "# Test connection\n",
    "curl http://localhost:6333\n",
    "```\n",
    "\n",
    "### Cloud Setup Issues (Option A)\n",
    "- Verify your Qdrant Cloud cluster is running in the dashboard\n",
    "- Double-check your cluster URL and API key\n",
    "- Make sure you're using the correct cluster region\n",
    "\n",
    "### Environment Variables Issues\n",
    "- Double-check your `.env` file is in the project root\n",
    "- Restart your Jupyter kernel after creating/updating `.env`\n",
    "- For local setup: `QDRANT_URL=http://localhost:6333` (no API key needed)\n",
    "- For cloud setup: Both `QDRANT_URL` and `QDRANT_API_KEY` required\n",
    "\n",
    "### OpenAI API Issues\n",
    "- Make sure you have credits in your OpenAI account\n",
    "- Verify your OpenAI API key is correct\n",
    "\n",
    "### Still Having Issues?\n",
    "- Check the `data/ingestion_summary.json` file (created after successful ingestion)\n",
    "- Look at the terminal output from the ingestion script for error messages\n",
    "- For Docker: Check Docker logs with `docker logs <container_id>`\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ Once you see \"Expected number of chunks found! Ingestion was successful.\" above, you're ready to continue!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Q/A Chatbot\n",
    "\n",
    "Now we can focus on the core RAG functionality without worrying about data preparation!\n",
    "\n",
    "![../imgs/naive-rag.png](../imgs/naive-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Retrieval - Search the cloud database for relevant embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T15:45:37.343026Z",
     "start_time": "2025-09-11T15:45:37.340999Z"
    }
   },
   "outputs": [],
   "source": [
    "def vector_search(query, top_k=2):\n",
    "    \"\"\"Search the Qdrant Cloud collection for relevant chunks.\"\"\"\n",
    "    # Create embedding of the query\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=query,\n",
    "        model=embedding_model\n",
    "    )\n",
    "    query_embeddings = response.data[0].embedding\n",
    "    \n",
    "    # Similarity search using the embedding\n",
    "    search_result = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embeddings,\n",
    "        with_payload=True,\n",
    "        limit=top_k,\n",
    "    ).points\n",
    "    \n",
    "    return [result.payload for result in search_result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generation - Use retrieved chunks to generate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T15:45:37.352721Z",
     "start_time": "2025-09-11T15:45:37.349446Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def model_generate(prompt, model=\"gpt-4o\"):\n",
    "    \"\"\"Generate response using OpenAI's chat completion.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # Deterministic output\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def prompt_template(question, context):\n",
    "    \"\"\"Create a prompt template for RAG.\"\"\"\n",
    "    return f\"\"\"You are an AI Assistant that provides answers to questions based on the following context. \n",
    "Make sure to only use the context to answer the question. Keep the wording very close to the context.\n",
    "\n",
    "Context:\n",
    "```\n",
    "{json.dumps(context)}\n",
    "```\n",
    "\n",
    "User question: {question}\n",
    "\n",
    "Answer in markdown:\"\"\"\n",
    "\n",
    "def generate_answer(question):\n",
    "    \"\"\"Complete RAG pipeline: retrieve and generate.\"\"\"\n",
    "    # Retrieval: search the knowledge base\n",
    "    search_result = vector_search(question)\n",
    "    if not search_result:\n",
    "        return \"No relevant information found.\"\n",
    "        \n",
    "    \n",
    "    # Generation: create prompt and generate answer\n",
    "    prompt = prompt_template(question, search_result)\n",
    "    return model_generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Test Basic RAG Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T15:46:02.190674Z",
     "start_time": "2025-09-11T15:45:37.359312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Question: When was BERT introduced and by which organization?\n",
      "\n",
      "üìö Retrieved Sources:\n",
      "1. BERT (language model)\n",
      "   Text: decoder, BERT can't be prompted and can't generate text, while bidirectional models in general do no...\n",
      "   Chunk 8 of 10\n",
      "\n",
      "\n",
      "2. BERT (language model)\n",
      "   Text: sentence. On October 25, 2019, Google announced that they had started applying BERT models for Engli...\n",
      "   Chunk 9 of 10\n",
      "\n",
      "\n",
      "3. BERT (language model)\n",
      "   Text: Bidirectional encoder representations from transformers (BERT) is a language model introduced in Oct...\n",
      "   Chunk 1 of 10\n",
      "\n",
      "\n",
      "\n",
      "ü§ñ Generated Answer:\n",
      "```markdown\n",
      "BERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Test with a clear, unambiguous question first\n",
    "question = \"When was BERT introduced and by which organization?\"\n",
    "search_result = vector_search(question, top_k=3)\n",
    "\n",
    "print(f\"üîç Question: {question}\")\n",
    "print(f\"\\nüìö Retrieved Sources:\")\n",
    "#print results in easy to read format\n",
    "for i, result in enumerate(search_result):\n",
    "    print(f\"{i+1}. {result['title']}\")\n",
    "    print(f\"   Text: {result['text'][:100]}...\")\n",
    "    print(f\"   Chunk {result['chunk_index']+1} of {result['total_chunks']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Generate answer\n",
    "answer = generate_answer(question)\n",
    "print(f\"\\nü§ñ Generated Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Evaluation with RAGAS\n",
    "\n",
    "Now let's evaluate our naive RAG system using **RAGAS** to establish baseline performance metrics and quantify the confusion we've observed.\n",
    "\n",
    "### Context-Focused Metrics:\n",
    "\n",
    "1. **Context Precision**: How well are relevant chunks ranked at the top?\n",
    "2. **Context Recall**: How much of the necessary information was retrieved?\n",
    "3. **Context Relevancy**: How relevant is the retrieved context to the question?\n",
    "\n",
    "We're using **RAGAS** because it's purpose-built for RAG evaluation and provides deep insights into context quality - the most critical component of RAG performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Evaluating your Naive RAG system with RAGAS...\n",
      "This will evaluate context quality metrics on 15 questions...\n",
      "\n",
      "‚úÖ Loaded 14 questions from evaluation dataset\n",
      "\n",
      "Evaluating 14 questions...\n",
      "\n",
      "Question 1/14: Who introduced the ReLU (rectified linear unit) ac...\n",
      "Question 2/14: What was the first working deep learning algorithm...\n",
      "Question 3/14: Which CNN achieved superhuman performance in a vis...\n",
      "Question 4/14: When was BERT introduced and by which organization...\n",
      "Question 5/14: What are the two model sizes BERT was originally i...\n",
      "Question 6/14: What percentage of tokens are randomly selected fo...\n",
      "Question 7/14: Who introduced the term 'deep learning' to the mac...\n",
      "Question 8/14: Which three researchers were awarded the 2018 Turi...\n",
      "Question 9/14: When was the first GPT introduced and by which org...\n",
      "Question 10/14: What were the three parameter sizes of the first v...\n",
      "Question 11/14: What is the 'one in ten rule' in regression analys...\n",
      "Question 12/14: What is the essence of overfitting according to th...\n",
      "Question 13/14: In which year and paper was the modern version of ...\n",
      "Question 14/14: What value did the original Transformer paper use ...\n",
      "\n",
      "üîç Running RAGAS evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a816f03d08304119804dfcc2c1b035d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RAGAS EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üìã INDIVIDUAL QUESTION SCORES:\n",
      "------------------------------------------------------------\n",
      " 1. üü¢ 1.000 - Who introduced the ReLU (rectified linear unit) activation f...\n",
      " 2. üü¢ 1.000 - What was the first working deep learning algorithm and who p...\n",
      " 3. üü¢ 1.000 - Which CNN achieved superhuman performance in a visual patter...\n",
      " 4. üî¥ 0.000 - When was BERT introduced and by which organization?\n",
      " 5. üü¢ 1.000 - What are the two model sizes BERT was originally implemented...\n",
      " 6. üü¢ 1.000 - What percentage of tokens are randomly selected for the mask...\n",
      " 7. üî¥ 0.000 - Who introduced the term 'deep learning' to the machine learn...\n",
      " 8. üî¥ 0.000 - Which three researchers were awarded the 2018 Turing Award f...\n",
      " 9. üü¢ 1.000 - When was the first GPT introduced and by which organization?\n",
      "10. üü¢ 1.000 - What were the three parameter sizes of the first versions of...\n",
      "11. üü¢ 1.000 - What is the 'one in ten rule' in regression analysis?\n",
      "12. üü¢ 1.000 - What is the essence of overfitting according to the article?\n",
      "13. üü¢ 1.000 - In which year and paper was the modern version of the transf...\n",
      "14. üü¢ 1.000 - What value did the original Transformer paper use for the pa...\n",
      "\n",
      "============================================================\n",
      "üìä AGGREGATE RESULTS\n",
      "============================================================\n",
      "\n",
      "CONTEXT RECALL METRIC (0.0 - 1.0 scale):\n",
      "  üü° Context Recall: 0.786\n",
      "============================================================\n",
      "\n",
      "üí° Tip: Add show_detailed=True to see full question details\n"
     ]
    }
   ],
   "source": [
    "# Import the RAGAS evaluation utility\n",
    "from rag_evaluator_v2 import evaluate_naive_rag_v2\n",
    "\n",
    "# Run evaluation on the current RAG system using RAGAS\n",
    "print(\"üîç Evaluating your Naive RAG system with RAGAS...\")\n",
    "print(\"This will evaluate context quality metrics on 15 questions...\\n\")\n",
    "\n",
    "baseline_results = evaluate_naive_rag_v2(\n",
    "    vector_search_func=vector_search,\n",
    "    generate_answer_func=generate_answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metrics': {'context_recall': 0.7857142857142857},\n",
       " 'aggregate_scores': {'context_recall': 0.7857142857142857,\n",
       "  'overall_context_score': 0.7857142857142857},\n",
       " 'individual_results': [{'question': 'Who introduced the ReLU (rectified linear unit) activation function and in what year?',\n",
       "   'generated_answer': '```markdown\\nKunihiko Fukushima introduced the ReLU (rectified linear unit) activation function in 1969.\\n```',\n",
       "   'ground_truth': 'Kunihiko Fukushima in 1969.',\n",
       "   'scores': {'context_recall': 1.0}},\n",
       "  {'question': 'What was the first working deep learning algorithm and who published it?',\n",
       "   'generated_answer': '```markdown\\nThe first working deep learning algorithm was the Group method of data handling, published by Alexey Ivakhnenko and Lapa in the Soviet Union in 1965.\\n```',\n",
       "   'ground_truth': 'The Group method of data handling, published by Alexey Ivakhnenko and Lapa in 1965.',\n",
       "   'scores': {'context_recall': 1.0}},\n",
       "  {'question': 'Which CNN achieved superhuman performance in a visual pattern recognition contest for the first time in 2011?',\n",
       "   'generated_answer': '```markdown\\nIn 2011, a CNN named DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J√ºrgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest.\\n```',\n",
       "   'ground_truth': 'DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J√ºrgen Schmidhuber.',\n",
       "   'scores': {'context_recall': 1.0}},\n",
       "  {'question': 'When was BERT introduced and by which organization?',\n",
       "   'generated_answer': '```markdown\\nBERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\n```\\n',\n",
       "   'ground_truth': 'October 2018 by researchers at Google.',\n",
       "   'scores': {'context_recall': 0.0}},\n",
       "  {'question': 'What are the two model sizes BERT was originally implemented in?',\n",
       "   'generated_answer': '```markdown\\nBERT was originally implemented in two model sizes: BERTBASE with 110 million parameters and BERTLARGE with 340 million parameters.\\n```',\n",
       "   'ground_truth': 'BERTBASE (110 million parameters) and BERTLARGE (340 million parameters).',\n",
       "   'scores': {'context_recall': 1.0}},\n",
       "  {'question': \"What percentage of tokens are randomly selected for the masked-prediction task in BERT's masked language modeling?\",\n",
       "   'generated_answer': \"```markdown\\nIn BERT's masked language modeling, 15% of tokens are randomly selected for the masked-prediction task.\\n```\",\n",
       "   'ground_truth': '15% of tokens.',\n",
       "   'scores': {'context_recall': 1.0}},\n",
       "  {'question': \"Who introduced the term 'deep learning' to the machine learning community and when?\",\n",
       "   'generated_answer': \"The provided context does not specify who introduced the term 'deep learning' to the machine learning community or when it was introduced.\",\n",
       "   'ground_truth': 'Rina Dechter in 1986.',\n",
       "   'scores': {'context_recall': 0.0}},\n",
       "  {'question': 'Which three researchers were awarded the 2018 Turing Award for their work on deep neural networks?',\n",
       "   'generated_answer': '```markdown\\nThe context provided does not contain information about the researchers who were awarded the 2018 Turing Award for their work on deep neural networks.\\n```',\n",
       "   'ground_truth': 'Yoshua Bengio, Geoffrey Hinton and Yann LeCun.',\n",
       "   'scores': {'context_recall': 0.0}},\n",
       "  {'question': 'When was the first GPT introduced and by which organization?',\n",
       "   'generated_answer': '```markdown\\nThe first GPT was introduced in 2018 by OpenAI.\\n```',\n",
       "   'ground_truth': '2018 by OpenAI.',\n",
       "   'scores': {'context_recall': 1.0}},\n",
       "  {'question': 'What were the three parameter sizes of the first versions of GPT-3 released in July 2020?',\n",
       "   'generated_answer': '```markdown\\nThe three parameter sizes of the first versions of GPT-3 released in July 2020 were 1B, 6.7B, and 175B, respectively named babbage, curie, and davinci.\\n```',\n",
       "   'ground_truth': '1B, 6.7B, and 175B parameters.',\n",
       "   'scores': {'context_recall': 1.0}},\n",
       "  {'question': \"What is the 'one in ten rule' in regression analysis?\",\n",
       "   'generated_answer': \"```markdown\\nThe 'one in ten rule' in regression analysis is a guideline that suggests having 10 observations per independent variable. This rule is used in the context of logistic regression or Cox proportional hazards models to help prevent overfitting.\\n```\\n\",\n",
       "   'ground_truth': 'The guideline of 10 observations per independent variable.',\n",
       "   'scores': {'context_recall': 1.0}},\n",
       "  {'question': 'What is the essence of overfitting according to the article?',\n",
       "   'generated_answer': '```markdown\\nThe essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.\\n```',\n",
       "   'ground_truth': 'To have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.',\n",
       "   'scores': {'context_recall': 1.0}},\n",
       "  {'question': 'In which year and paper was the modern version of the transformer proposed?',\n",
       "   'generated_answer': '```markdown\\nThe modern version of the transformer was proposed in the 2017 paper \"Attention Is All You Need\" by researchers at Google.\\n```',\n",
       "   'ground_truth': '2017 in the paper \"Attention Is All You Need\" by researchers at Google.',\n",
       "   'scores': {'context_recall': 1.0}},\n",
       "  {'question': 'What value did the original Transformer paper use for the parameter N in positional encoding?',\n",
       "   'generated_answer': '```markdown\\nThe original Transformer paper used \\\\( N = 10000 \\\\) for the parameter in positional encoding.\\n```',\n",
       "   'ground_truth': 'N = 10000.',\n",
       "   'scores': {'context_recall': 1.0}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
