{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RAG Workshop Notebook - Naive RAG\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:19:59.446024Z",
     "start_time": "2025-03-08T12:19:58.360870Z"
    }
   },
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install wikipedia mwparserfromhell beautifulsoup4 openai qdrant-client tqdm python-dotenv\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (25.0.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wikipedia in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (1.4.0)\r\n",
      "Requirement already satisfied: mwparserfromhell in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (0.6.6)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (4.13.3)\r\n",
      "Requirement already satisfied: openai in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (1.61.1)\r\n",
      "Requirement already satisfied: qdrant-client in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (1.13.2)\r\n",
      "Requirement already satisfied: tqdm in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (4.67.1)\r\n",
      "Requirement already satisfied: python-dotenv in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from wikipedia) (2.32.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from beautifulsoup4) (2.6)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from beautifulsoup4) (4.12.2)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from openai) (4.8.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from openai) (0.28.1)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from openai) (0.8.2)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from openai) (2.10.6)\r\n",
      "Requirement already satisfied: sniffio in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from openai) (1.3.1)\r\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from qdrant-client) (1.70.0)\r\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from qdrant-client) (1.70.0)\r\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from qdrant-client) (1.26.4)\r\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from qdrant-client) (2.10.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from qdrant-client) (2.3.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\r\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.29.3)\r\n",
      "Requirement already satisfied: setuptools in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (68.2.0)\r\n",
      "Requirement already satisfied: certifi in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\r\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\r\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /Users/sarang/projects/personal/building-rag-app-workshop/.venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:19:59.455367Z",
     "start_time": "2025-03-08T12:19:59.451770Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion:\n",
    "1. Fetch Wikipedia articles using the Wikipedia API.\n",
    "2. Clean the text by removing wiki markup and citation numbers.\n",
    "3. Chunk the text into smaller pieces to create embeddings.\n",
    "4. Create embeddings using OpenAI's text-embedding-3-small model.\n",
    "5. Index the embeddings using Qdrant Vector Store.\n",
    "\n",
    "![../imgs/ingestion.png](../imgs/ingestion.png)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1. Fetch Wikipedia articles using the Wikipedia API."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.123746Z",
     "start_time": "2025-03-08T12:19:59.463154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wikipedia\n",
    "import re\n",
    "from mwparserfromhell import parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "ARTICLE_TITLES = [\n",
    "    \"Deep learning\",\n",
    "    \"Transformer (machine learning model)\",\n",
    "    \"Natural language processing\",\n",
    "    \"Reinforcement learning\",\n",
    "    \"Artificial neural network\",\n",
    "    \"Generative pre-trained transformer\",\n",
    "    \"BERT (language model)\", \"Overfitting\"\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_wikipedia_article(title):\n",
    "    try:\n",
    "        page = wikipedia.page(title)\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"url\": page.url,\n",
    "            \"raw_content\": page.content\n",
    "        }\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return fetch_wikipedia_article(e.options[0])\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"Skipping {title}\")\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2. Clean the text by removing wiki markup and citation numbers."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text(text):\n",
    "    # Remove wiki markup and citation numbers\n",
    "    text = ''.join(parse(text).strip_code())\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    return re.sub(r'\\[\\d+\\]', '', text).strip()\n",
    "\n",
    "\n",
    "articles = []\n",
    "for title in ARTICLE_TITLES:\n",
    "    article = fetch_wikipedia_article(title)\n",
    "    if article:\n",
    "        article[\"content\"] = clean_text(article[\"raw_content\"])\n",
    "        articles.append(article)\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "articles[1]['content'][:1000]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3. Chunk the text into smaller pieces to create embeddings.\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.149160Z",
     "start_time": "2025-03-08T12:20:14.142830Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 50,
   "source": [
    "# Chunking function\n",
    "def chunk_text(text, chunk_size=1000, overlap=100):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size])\n",
    "            for i in range(0, len(words), chunk_size - overlap)]\n",
    "\n",
    "\n",
    "# Prepare chunks and metadata\n",
    "corpus = []\n",
    "metadata = []\n",
    "for article in articles:\n",
    "    chunks = chunk_text(article[\"content\"])\n",
    "    corpus.extend(chunks)\n",
    "    metadata.extend([{\"title\": article[\"title\"], \"url\": article[\"url\"]}] * len(chunks))"
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.158827Z",
     "start_time": "2025-03-08T12:20:14.156223Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Corpus: 52\n",
      "Total Metadata: 52\n"
     ]
    }
   ],
   "execution_count": 51,
   "source": [
    "print('Total Corpus:', len(corpus))\n",
    "print('Total Metadata:', len(metadata))\n",
    "\n",
    "deep_learning_chunks = [chunk for chunk, meta in zip(corpus, metadata) if meta['title'] == 'Deep learning']"
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.177771Z",
     "start_time": "2025-03-08T12:20:14.175500Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52,
   "source": "len(deep_learning_chunks)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.4. Create embeddings using OpenAI's text-embedding-3-small model."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:14.196735Z",
     "start_time": "2025-03-08T12:20:14.190270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "# Define the embedding function using OpenAI's API (using text-embedding-ada-002)\n",
    "def openai_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=[text],  # Passing the text as a list\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    # Use dot notation to access the embedding from the response object\n",
    "    embeddings = [data.embedding for data in response.data]\n",
    "    return embeddings"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.106911Z",
     "start_time": "2025-03-08T12:20:14.201619Z"
    }
   },
   "source": [
    "embeddings = []\n",
    "chunked_texts = []\n",
    "metadata_chunks = []\n",
    "test_corpus = corpus[:10]\n",
    "\n",
    "for chunk in tqdm(test_corpus):\n",
    "    embedding = openai_embedding(chunk)\n",
    "    embeddings.extend(embedding)\n",
    "    chunked_texts.extend([chunk] * len(embedding))\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 1.5. Index the embeddings using Qdrant Vector Store.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.136433Z",
     "start_time": "2025-03-08T12:20:20.121187Z"
    }
   },
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "\n",
    "# Create an in-memory Qdrant instance\n",
    "client = QdrantClient(\":memory:\")\n",
    "collection_name = \"wikipedia_articles\"\n",
    "\n",
    "# Create the collection with the specified vector configuration\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Upsert points into the collection using PointStruct for each point\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=idx,\n",
    "            vector=embedding,\n",
    "            payload={\"text\": chunked_texts[idx]}\n",
    "        )\n",
    "        for idx, embedding in enumerate(embeddings)\n",
    "    ]\n",
    ")\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Build the Q/A Chatbot\n",
    "\n",
    "![../imgs/naive-rag.png](../imgs/naive-rag.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1. Retrieval - Search the database for the most relevant embeddings."
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.471716Z",
     "start_time": "2025-03-08T12:20:20.142631Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Deep learning is a subset of machine learning that focuses on '\n",
      "         'utilizing neural networks to perform tasks such as classification, '\n",
      "         'regression, and representation learning. The field takes inspiration '\n",
      "         'from biological neuroscience and is centered around stacking '\n",
      "         'artificial neurons into layers and \"training\" them to process data. '\n",
      "         'The adjective \"deep\" refers to the use of multiple layers (ranging '\n",
      "         'from three to several hundred or thousands) in the network. Methods '\n",
      "         'used can be either supervised, semi-supervised or unsupervised. Some '\n",
      "         'common deep learning network architectures include fully connected '\n",
      "         'networks, deep belief networks, recurrent neural networks, '\n",
      "         'convolutional neural networks, generative adversarial networks, '\n",
      "         'transformers, and neural radiance fields. These architectures have '\n",
      "         'been applied to fields including computer vision, speech '\n",
      "         'recognition, natural language processing, machine translation, '\n",
      "         'bioinformatics, drug design, medical image analysis, climate '\n",
      "         'science, material inspection and board game programs, where they '\n",
      "         'have produced results comparable to and in some cases surpassing '\n",
      "         'human expert performance. Early forms of neural networks were '\n",
      "         'inspired by information processing and distributed communication '\n",
      "         'nodes in biological systems, particularly the human brain. However, '\n",
      "         'current neural networks do not intend to model the brain function of '\n",
      "         'organisms, and are generally seen as low-quality models for that '\n",
      "         'purpose. Overview Most modern deep learning models are based on '\n",
      "         'multi-layered neural networks such as convolutional neural networks '\n",
      "         'and transformers, although they can also include propositional '\n",
      "         'formulas or latent variables organized layer-wise in deep generative '\n",
      "         'models such as the nodes in deep belief networks and deep Boltzmann '\n",
      "         'machines. Fundamentally, deep learning refers to a class of machine '\n",
      "         'learning algorithms in which a hierarchy of layers is used to '\n",
      "         'transform input data into a progressively more abstract and '\n",
      "         'composite representation. For example, in an image recognition '\n",
      "         'model, the raw input may be an image (represented as a tensor of '\n",
      "         'pixels). The first representational layer may attempt to identify '\n",
      "         'basic shapes such as lines and circles, the second layer may compose '\n",
      "         'and encode arrangements of edges, the third layer may encode a nose '\n",
      "         'and eyes, and the fourth layer may recognize that the image contains '\n",
      "         'a face. Importantly, a deep learning process can learn which '\n",
      "         'features to optimally place at which level on its own. Prior to deep '\n",
      "         'learning, machine learning techniques often involved hand-crafted '\n",
      "         'feature engineering to transform the data into a more suitable '\n",
      "         'representation for a classification algorithm to operate on. In the '\n",
      "         'deep learning approach, features are not hand-crafted and the model '\n",
      "         'discovers useful feature representations from the data '\n",
      "         'automatically. This does not eliminate the need for hand-tuning; for '\n",
      "         'example, varying numbers of layers and layer sizes can provide '\n",
      "         'different degrees of abstraction. The word \"deep\" in \"deep learning\" '\n",
      "         'refers to the number of layers through which the data is '\n",
      "         'transformed. More precisely, deep learning systems have a '\n",
      "         'substantial credit assignment path (CAP) depth. The CAP is the chain '\n",
      "         'of transformations from input to output. CAPs describe potentially '\n",
      "         'causal connections between input and output. For a feedforward '\n",
      "         'neural network, the depth of the CAPs is that of the network and is '\n",
      "         'the number of hidden layers plus one (as the output layer is also '\n",
      "         'parameterized). For recurrent neural networks, in which a signal may '\n",
      "         'propagate through a layer more than once, the CAP depth is '\n",
      "         'potentially unlimited. No universally agreed-upon threshold of depth '\n",
      "         'divides shallow learning from deep learning, but most researchers '\n",
      "         'agree that deep learning involves CAP depth higher than two. CAP of '\n",
      "         'depth two has been shown to be a universal approximator in the sense '\n",
      "         'that it can emulate any function. Beyond that, more layers do not '\n",
      "         'add to the function approximator ability of the network. Deep models '\n",
      "         '(CAP > two) are able to extract better features than shallow models '\n",
      "         'and hence, extra layers help in learning the features effectively. '\n",
      "         'Deep learning architectures can be constructed with a greedy '\n",
      "         'layer-by-layer method. Deep learning helps to disentangle these '\n",
      "         'abstractions and pick out which features improve performance. Deep '\n",
      "         'learning algorithms can be applied to unsupervised learning tasks. '\n",
      "         'This is an important benefit because unlabeled data are more '\n",
      "         'abundant than the labeled data. Examples of deep structures that can '\n",
      "         'be trained in an unsupervised manner are deep belief networks. The '\n",
      "         'term Deep Learning was introduced to the machine learning community '\n",
      "         'by Rina Dechter in 1986, and to artificial neural networks by Igor '\n",
      "         'Aizenberg and colleagues in 2000, in the context of Boolean '\n",
      "         'threshold neurons. Although the history of its appearance is '\n",
      "         'apparently more complicated. Interpretations Deep neural networks '\n",
      "         'are generally interpreted in terms of the universal approximation '\n",
      "         'theorem or probabilistic inference. The classic universal '\n",
      "         'approximation theorem concerns the capacity of feedforward neural '\n",
      "         'networks with a single hidden layer of finite size to approximate '\n",
      "         'continuous functions. In 1989, the first proof was published by '\n",
      "         'George Cybenko for sigmoid activation functions and was generalised '\n",
      "         'to feed-forward multi-layer architectures in 1991 by Kurt Hornik. '\n",
      "         'Recent work also showed that universal approximation also holds for '\n",
      "         \"non-bounded activation functions such as Kunihiko Fukushima's \"\n",
      "         'rectified linear unit. The universal approximation theorem for deep '\n",
      "         'neural networks concerns the capacity of networks with bounded width '\n",
      "         'but the depth is allowed to grow. Lu et al. proved that if the width '\n",
      "         'of a deep neural network with ReLU activation is strictly larger '\n",
      "         'than the input dimension, then the network can approximate any '\n",
      "         'Lebesgue integrable function; if the width is smaller or equal to '\n",
      "         'the input dimension, then a deep neural network is not a universal '\n",
      "         'approximator. The probabilistic interpretation derives from the '\n",
      "         'field of machine learning. It features inference, as well as the '\n",
      "         'optimization concepts of training and testing, related to fitting '\n",
      "         'and generalization, respectively. More specifically, the '\n",
      "         'probabilistic interpretation considers the activation nonlinearity '\n",
      "         'as a cumulative distribution function. The probabilistic '\n",
      "         'interpretation led to the introduction of dropout as regularizer in '\n",
      "         'neural networks. The probabilistic interpretation was introduced by '\n",
      "         'researchers including Hopfield, Widrow and Narendra and popularized '\n",
      "         'in surveys such as the one by Bishop. History Before 1980 There are '\n",
      "         'two types of artificial neural network (ANN): feedforward neural '\n",
      "         'network (FNN) or multilayer perceptron (MLP) and recurrent neural '\n",
      "         'networks (RNN). RNNs have cycles in their connectivity'}\n"
     ]
    }
   ],
   "execution_count": 56,
   "source": [
    "# Function to search the database\n",
    "def vector_search(query, top_k=3):\n",
    "    # create embedding of the query\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    query_embeddings = response.data[0].embedding\n",
    "    # similarity search using the embedding, give top n results which are close to the query embeddings\n",
    "    search_result = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embeddings,\n",
    "        with_payload=True,\n",
    "        limit=top_k,\n",
    "    ).points\n",
    "    return [result.payload for result in search_result]\n",
    "\n",
    "\n",
    "search_result = vector_search(\"What does the word 'deep' in 'deep learning' refer\")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(search_result[0])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2. Generation - Use the retrieved embeddings to generate the answer."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.533382Z",
     "start_time": "2025-03-08T12:20:20.524889Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 57,
   "source": [
    "def model_generate(prompt, model=\"gpt-4o-mini\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,  # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def prompt_template(question, context):\n",
    "    return \"\"\"You are a AI Assistant that provides answer to the question at the end, over the following\n",
    "  pieces of context. Make sure to only use the context to answer the question. Keep the wording very close to the context\n",
    "  context:\n",
    "  ```\n",
    "  \"\"\" + json.dumps(context) + \"\"\"\n",
    "  ```\n",
    "  User question: \"\"\" + question + \"\"\"\n",
    "  Answer in markdown:\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:20.569179Z",
     "start_time": "2025-03-08T12:20:20.560036Z"
    }
   },
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_answer(question):\n",
    "    #Retrieval: search a knowledge base.\n",
    "    search_result = vector_search(question)\n",
    "\n",
    "    prompt = prompt_template(question, search_result)\n",
    "    # Generation: LLMs' ability to generate the answer\n",
    "    return model_generate(prompt)\n",
    "\n",
    "\n",
    "question = f\"What is A common evaluation set for image classification? \"\n",
    "answer = generate_answer(question)\n",
    "print(\"Answer:\", answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:21.903054Z",
     "start_time": "2025-03-08T12:20:20.582952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples.\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "source": [
    "question = f\"Who introduced the time delay neural network (TDNN)? and when ?\"\n",
    "answer = generate_answer(question)\n",
    "print(\"Answer:\", answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-08T12:45:49.559550Z",
     "start_time": "2025-03-08T12:45:46.631155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The time delay neural network (TDNN) was introduced by Alex Waibel in 1987.\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T12:20:21.936317Z",
     "start_time": "2025-03-08T12:20:21.934810Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
